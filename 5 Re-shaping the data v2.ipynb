{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "24c6c5a5-b1fb-462f-8506-8f2de70495a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "**Outpatient Pipeline Summary (Steps 9-14)**\n",
    "\n",
    "Click to expand for full details.\n",
    "\n",
    "This pipeline processes outpatient activity data into a clean, multi-level dataset ready for analysis, benchmarking, and reporting.\n",
    "\n",
    "Step 9 Wide Table & Provider Mapping\n",
    "The raw outpatient snapshot is filtered for relevant years, attendance types, and administrative categories. Treatment Function Codes are validated and grouped, and key metrics (totals, first/follow-up appointments, procedures, DNAs, 2WW) are aggregated by month, provider, and treatment group. Provider mergers are applied, and ICB and region codes are mapped for organisational context. Month-end dates are derived for consistent time-series analysis.\n",
    "\n",
    "Step 10 & 10A Derived Metrics & Benchmarks\n",
    "Key performance metrics (e.g., DNA rates, procedure percentages, remote vs face-to-face ratios) are calculated safely to handle missing data. The 25th percentile of remote activity is computed as a lower benchmark, allowing providers to see where they sit relative to peers.\n",
    "\n",
    "Step 11 & 13 Long / OPRT Format\n",
    "Metrics are reshaped into a long format, turning each measure into a key–value pair while retaining identifiers like month, provider, ICB, region, and treatment group. This structure supports flexible reporting, visualization, and dashboard integration.\n",
    "\n",
    "Step 12 Multi-Level Aggregation\n",
    "Metrics are aggregated across Organisation, ICB, and Region levels. Derived rates are recalculated for each level, producing a consistent view of performance from local to regional scale.\n",
    "\n",
    "Step 14 Internal Metric IDs\n",
    "Each metric is linked with its Treatment Function Group and assigned an InternalID for consistent tracking and reporting. The final dataset is saved in Parquet format for downstream use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3069d1c2-d996-422e-9bb1-cc35dedef6c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1 Importing Tools \n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "from openpyxl.styles import NamedStyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fc08c57-eb66-4838-8238-ae0b006f0ce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2 Reduce risk of a timeout by increasing limit to 30 minutes\n",
    "spark.conf.set(\"spark.databricks.execution.timeout\", \"1800\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d21a9981-c2a1-46a2-9e9b-419c78eea302",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3 Loading the master hierarchies table from the lake mart\n",
    "df_master_hierarchies = spark.read.option(\"header\", \"true\").csv(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/EROC_Collection_Queries/master_hierarchies_table.csv\")\n",
    "display(df_master_hierarchies.limit(10))\n",
    "print(f\"Number of rows in master hierarchies: {df_master_hierarchies.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6334b52-eb66-4ef5-a95c-afea7c3e7ea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#4 loading ICB to Region table\n",
    "df_icb_region = spark.read.option(\"header\", \"true\").csv(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/EROC_Collection/EROC/EROC_ICB_Region_DisplayNames.csv\")  # Ensure proper Azure credentials are configured for ADLS access.\n",
    "display(df_icb_region.limit(10))\n",
    "print(f\"Number of rows in icb_region: {df_icb_region.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a9b0856-5c30-47d8-8e98-a08bab8926f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#5 loading list of merged providers\n",
    "df_merged_providers = spark.read.option(\"header\", \"true\").csv(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/EROC_Collection/EROC/EROC_Merged_Providers.csv\")\n",
    "display(df_merged_providers.limit(10))\n",
    "print(f\"Number of rows in merged providers: {df_merged_providers.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a08aeef7-8c39-402c-be16-83f587b7c999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#6 creating new provider code from the provider mapping table\n",
    "provider_code_mapping = df_merged_providers = spark.read.option(\"header\", \"true\").csv(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/EROC_Collection/EROC/EROC_Merged_Providers.csv\")\n",
    "display(df_merged_providers.limit(10))\n",
    "print(f\"Number of rows in merged providers: {df_merged_providers.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "373bf4a4-013f-42f6-81eb-76f55d979cd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#7 importing MHS metric list and internal ID\n",
    "mhs_metric_list = spark.read.option(\"header\", \"true\").csv(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/EROC_Collection/MHS\")\n",
    "display(mhs_metric_list.limit(10))\n",
    "print(f\"Number of rows in mhs_metric_list: {mhs_metric_list.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c2a5552-3d8d-4900-8371-c80952cf4b03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#8 Loading the core monthly snapshot data\n",
    "from pyspark.sql import functions as F\n",
    "df_op_activity_snapshot = spark.read.option(\"header\", \"true\").option(\"recursiveFileLookup\", \"true\").parquet(\n",
    "    \"abfss://reporting@udalstdatacuratedprod.dfs.core.windows.net/restricted/patientlevel/MESH/OPA/OPA_Core_Monthly_Snapshot/Published/1\"\n",
    ")\n",
    "display(df_op_activity_snapshot.limit(10))\n",
    "\n",
    "# Show number of rows in the raw data\n",
    "row_count = df_op_activity_snapshot.count()\n",
    "print(f\"Number of rows in raw data: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9b4e4da0-33d4-45b3-b603-d9166e5c18fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "**Step 9 Creating the Wide Table and Provider Mapping**\n",
    "\n",
    "Click to expand for full details.\n",
    "\n",
    "Builds a wide, aggregated outpatient activity table from the snapshot dataset. Treatment Function Codes are validated and grouped, and the data is filtered for relevant years, attendance types, and administrative categories. Key metrics are aggregated by month, provider, and treatment group, including totals, first/follow-up, procedures, DNAs, and 2WW appointments. The table also accounts for provider mergers, maps to ICB and Region codes, and derives month-end dates, producing a clean, regionally aligned dataset ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f82af93-37d4-4281-b468-6e017347849f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#9 Creating the wide table & inserting new column for merged providers with new merger codes and mapping to ICB and Region codes\n",
    "from pyspark.sql.functions import when, col, lit, create_map, coalesce\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Define valid treatment function codes\n",
    "VALID_TREATMENT_CODES = [\n",
    "    '100', '101', '102', '104', '105', '106', '108', '110', '111', '115', '120', '130', '140',\n",
    "    '144', '145', '301', '302', '303', '307', '320', '330', '340', '361', '400', '410', '420',\n",
    "    '430', '501', '502', '560', '650'\n",
    "]\n",
    "\n",
    "# Adding in the Treatment_Function_Code_New column\n",
    "opa_with_tfc = df_op_activity_snapshot.withColumn(\n",
    "    \"Treatment_Function_Code_New\",\n",
    "    when(col(\"Treatment_Function_Code\").isin(VALID_TREATMENT_CODES), col(\"Treatment_Function_Code\")).otherwise(\"Other\")\n",
    ")\n",
    "\n",
    "# Add Treatment_Function_Group column using VALID_TREATMENT_CODES groupings\n",
    "opa_with_groups = opa_with_tfc.withColumn(\n",
    "    \"Treatment_Function_Group\",\n",
    "    when(col(\"Treatment_Function_Code_New\").isin(\"100\", \"102\", \"104\", \"105\", \"106\"), \"GS\")\n",
    "     .when(col(\"Treatment_Function_Code_New\").isin(\"140\", \"144\", \"145\"), \"OMFS\")\n",
    "     .when(col(\"Treatment_Function_Code_New\").isin(\"110\", \"111\", \"115\"), \"T&O\")\n",
    "     .otherwise(col(\"Treatment_Function_Code_New\"))\n",
    ")\n",
    "\n",
    "# Filter dataset for relevant years, admin category, TFC, and attendance\n",
    "opa_filtered = opa_with_groups.filter(\n",
    "    (col(\"Der_Financial_Year\").isin(\"2023/24\", \"2024/25\", \"2025/26\")) &  # This can be updated manually\n",
    "    (col(\"Administrative_Category\") == \"01\") &\n",
    "    (col(\"Treatment_Function_Code\") != \"812\") &\n",
    "    (col(\"First_Attendance\").isin(\"1\", \"2\", \"3\", \"4\"))\n",
    ")\n",
    "\n",
    "# Aggregates the metrics by month, provider, and Treatment_Function_Group\n",
    "opa_agg = opa_filtered.groupBy(\n",
    "    \"Der_Activity_Month\",\n",
    "    \"Der_Provider_Code\",\n",
    "    \"Treatment_Function_Group\"\n",
    ").agg(\n",
    "    # All contacts\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"1\", \"2\", \"3\", \"4\")), 1).otherwise(0)).alias(\"All_Total\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"1\", \"3\")), 1).otherwise(0)).alias(\"All_First\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"2\", \"4\")), 1).otherwise(0)).alias(\"All_FU\"),\n",
    "    F.sum(when((col(\"Der_Number_Procedure\") > 0) & (col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"1\", \"2\", \"3\", \"4\")), 1).otherwise(0)).alias(\"All_Proc\"),\n",
    "    F.sum(when((col(\"Der_Number_Procedure\") == 0) & (col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"1\", \"2\", \"3\", \"4\")), 1).otherwise(0)).alias(\"All_NoProc\"),\n",
    "    F.sum(when((col(\"Der_Number_Procedure\") > 0) & (col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"2\", \"4\")), 1).otherwise(0)).alias(\"All_FU_Proc\"),\n",
    "    F.sum(when((col(\"Der_Number_Procedure\") == 0) & (col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"2\", \"4\")), 1).otherwise(0)).alias(\"All_FU_NoProc\"),\n",
    "    # Face-to-face\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"1\", \"2\")), 1).otherwise(0)).alias(\"F2F_Total\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\") == \"1\"), 1).otherwise(0)).alias(\"F2F_First\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\") == \"2\"), 1).otherwise(0)).alias(\"F2F_FU\"),\n",
    "    # Remote\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"3\", \"4\")), 1).otherwise(0)).alias(\"Remote_Total\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\") == \"3\"), 1).otherwise(0)).alias(\"Remote_First\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\") == \"4\"), 1).otherwise(0)).alias(\"Remote_FU\"),\n",
    "    # Did not attends (DNAs)\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"3\", \"7\")) & (col(\"First_Attendance\").isin(\"1\", \"2\", \"3\", \"4\")), 1).otherwise(0)).alias(\"All_DNA\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"3\", \"7\")) & (col(\"First_Attendance\").isin(\"1\", \"3\")), 1).otherwise(0)).alias(\"All_First_DNA\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"3\", \"7\")) & (col(\"First_Attendance\").isin(\"2\", \"4\")), 1).otherwise(0)).alias(\"All_FU_DNA\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"3\", \"7\")) & (col(\"First_Attendance\").isin(\"1\", \"2\")), 1).otherwise(0)).alias(\"F2F_DNA\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"3\", \"7\")) & (col(\"First_Attendance\").isin(\"3\", \"4\")), 1).otherwise(0)).alias(\"Remote_DNA\"),\n",
    "    # 2WW DNA\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"3\", \"7\")) & (col(\"First_Attendance\").isin(\"1\", \"2\", \"3\", \"4\")) & (col(\"Priority_Type\") == \"3\"), 1).otherwise(0)).alias(\"All_2WW_DNA\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"3\", \"7\")) & (col(\"First_Attendance\").isin(\"1\", \"3\")) & (col(\"Priority_Type\") == \"3\"), 1).otherwise(0)).alias(\"All_First_2WW_DNA\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"3\", \"7\")) & (col(\"First_Attendance\").isin(\"2\", \"4\")) & (col(\"Priority_Type\") == \"3\"), 1).otherwise(0)).alias(\"All_FU_2WW_DNA\"),\n",
    "    # All 2WW appointments\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\", \"3\", \"7\")) & (col(\"First_Attendance\").isin(\"1\", \"2\", \"3\", \"4\")) & (col(\"Priority_Type\") == \"3\"), 1).otherwise(0)).alias(\"All_2WW\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\", \"3\", \"7\")) & (col(\"First_Attendance\").isin(\"1\", \"3\")) & (col(\"Priority_Type\") == \"3\"), 1).otherwise(0)).alias(\"All_First_2WW\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\", \"3\", \"7\")) & (col(\"First_Attendance\").isin(\"2\", \"4\")) & (col(\"Priority_Type\") == \"3\"), 1).otherwise(0)).alias(\"All_FU_2WW\")\n",
    ")\n",
    "\n",
    "# Add \"All\" TFC totals by month and provider\n",
    "METRIC_COLS = [c for c in opa_agg.columns if c not in [\"Der_Activity_Month\", \"Der_Provider_Code\", \"Treatment_Function_Group\"]]\n",
    "\n",
    "opa_all_tfc = opa_agg.groupBy(\"Der_Activity_Month\", \"Der_Provider_Code\").agg(\n",
    "    *[F.sum(col(c)).alias(c) for c in METRIC_COLS]\n",
    ").withColumn(\"Treatment_Function_Group\", lit(\"All\"))\n",
    "\n",
    "opa_final = opa_agg.unionByName(opa_all_tfc)\n",
    "\n",
    "# Order by results\n",
    "opa_final_ordered = opa_final.orderBy(\"Der_Activity_Month\", \"Der_Provider_Code\", \"Treatment_Function_Group\")\n",
    "\n",
    "# Inserted mapping code to build mapping_expr from df_merged_providers\n",
    "provider_code_mapping_dict = {\n",
    "    row['Old_Provider_Code']: row['New_Provider_Code']\n",
    "    for row in df_merged_providers.select(\"Old_Provider_Code\", \"New_Provider_Code\").distinct().collect()\n",
    "}\n",
    "\n",
    "mapping_list = []\n",
    "for k, v in provider_code_mapping_dict.items():\n",
    "    mapping_list.append(lit(k))\n",
    "    mapping_list.append(lit(v))\n",
    "\n",
    "mapping_expr = create_map(mapping_list)\n",
    "\n",
    "# Add \"Adj Org Code\" column based on provider_code_mapping\n",
    "opa_final_ordered_with_adj = opa_final_ordered.withColumn(\n",
    "    \"Adj Org Code\",\n",
    "    coalesce(mapping_expr.getItem(col(\"Der_Provider_Code\")), col(\"Der_Provider_Code\"))\n",
    ")\n",
    "\n",
    "# Add \"ICB\" column by joining to df_master_hierarchies on Organisation_Code and returning STP_Code\n",
    "opa_final_ordered_with_icb = opa_final_ordered_with_adj.join(\n",
    "    df_master_hierarchies.select(\n",
    "        F.col(\"Organisation_Code\").alias(\"join_org_code\"),\n",
    "        F.col(\"STP_Code\").alias(\"ICB\")\n",
    "    ),\n",
    "    opa_final_ordered_with_adj[\"Adj Org Code\"] == F.col(\"join_org_code\"),\n",
    "    \"left\"\n",
    ").drop(\"join_org_code\")\n",
    "\n",
    "# Add \"Region\" column by joining to df_icb_region on ICB column and returning Region_Code\n",
    "opa_final_ordered_with_icb_region = opa_final_ordered_with_icb.join(\n",
    "    df_icb_region.select(\n",
    "        F.col(\"ICB_Code\").alias(\"join_icb\"),\n",
    "        F.col(\"Region_Code\")\n",
    "    ),\n",
    "    opa_final_ordered_with_icb[\"ICB\"] == F.col(\"join_icb\"),\n",
    "    \"left\"\n",
    ").drop(\"join_icb\")\n",
    "\n",
    "from pyspark.sql.functions import last_day, to_date, concat_ws\n",
    "\n",
    "opa_final_ordered_with_icb_region = opa_final_ordered_with_icb_region.withColumn(\n",
    "    \"Der_Activity_Month_Date\",\n",
    "    last_day(\n",
    "        to_date(\n",
    "            concat_ws(\n",
    "                '-',\n",
    "                col(\"Der_Activity_Month\").substr(1, 4),\n",
    "                col(\"Der_Activity_Month\").substr(5, 2),\n",
    "                lit(\"01\")\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "#display(opa_final_ordered_with_icb_region.limit(10))\n",
    "\n",
    "opa_final_ordered_with_icb_region_row_count = opa_final_ordered_with_icb_region.count()\n",
    "# Drop unwanted columns, aggregate metrics, and sort the final table\n",
    "id_cols = [\"Der_Activity_Month_Date\", \"Treatment_Function_Group\", \"Region_Code\", \"ICB\", \"Adj Org Code\"]\n",
    "\n",
    "# Determine metric columns (exclude identifiers and the three columns to drop)\n",
    "metric_cols = [\n",
    "    c for c in opa_final_ordered_with_icb_region.columns\n",
    "    if c not in id_cols + [\"Der_Activity_Month\", \"Der_Provider_Code\", \"Treatment_Function_Code_New\"]\n",
    "]\n",
    "\n",
    "opa_final_processed = (\n",
    "    opa_final_ordered_with_icb_region\n",
    "    .groupBy(*[F.col(c) for c in id_cols])\n",
    "    .agg(*[F.sum(F.col(c)).alias(c) for c in metric_cols])\n",
    "    .orderBy(\"Der_Activity_Month_Date\", \"Region_Code\", \"ICB\", \"Adj Org Code\", \"Treatment_Function_Group\")\n",
    ")\n",
    "\n",
    "display(opa_final_processed.limit(10))\n",
    "print(f\"Number of rows in opa_final_processed: {opa_final_processed.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c2ab3e5c-e02d-460a-b785-a9cedf7b7525",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "**For step 10 summary**\n",
    "\n",
    "Click to expand for full details.\n",
    "\n",
    "This step enhances the outpatient wide table with additional performance metrics while ensuring robustness against missing columns. A reusable safe_add function checks column availability before computing new ratios (e.g., DNA %, procedure %, remote %, and face-to-face rates). The script also creates duplicate columns for downstream compatibility and safely aggregates related measures into combined totals. By using conditional logic and dynamic column creation, this process guarantees consistent metric generation even when source fields are incomplete or evolve over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab1177ed-00ad-44f3-8eda-776b6093dd07",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760706170841}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 10 Safe metric calculation (robust to missing columns)\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = opa_final_ordered_with_icb_region\n",
    "\n",
    "# Helper to safely add derived columns only if dependencies exist\n",
    "def safe_add(df, new_col, expr_fn, required_cols):\n",
    "    if all(c in df.columns for c in required_cols):\n",
    "        return df.withColumn(new_col, expr_fn(df))\n",
    "    else:\n",
    "        return df.withColumn(new_col, F.lit(None))\n",
    "\n",
    "# Now build metrics safely\n",
    "metrics = [\n",
    "    (\"All_DNA_Over_All_Total\", lambda d: F.when((F.col(\"All_Total\")+F.col(\"All_DNA\"))!=0,\n",
    "                                                (F.col(\"All_DNA\")/(F.col(\"All_Total\")+F.col(\"All_DNA\")))*100),\n",
    "     [\"All_Total\", \"All_DNA\"]),\n",
    "    (\"All_First_DNA_Over_All_First\", lambda d: F.when((F.col(\"All_First\")+F.col(\"All_First_DNA\"))!=0,\n",
    "                                                      (F.col(\"All_First_DNA\")/(F.col(\"All_First\")+F.col(\"All_First_DNA\")))*100),\n",
    "     [\"All_First\", \"All_First_DNA\"]),\n",
    "    (\"All_FU_DNA_Over_All_FU\", lambda d: F.when((F.col(\"All_FU\")+F.col(\"All_FU_DNA\"))!=0,\n",
    "                                                (F.col(\"All_FU_DNA\")/(F.col(\"All_FU\")+F.col(\"All_FU_DNA\")))*100),\n",
    "     [\"All_FU\", \"All_FU_DNA\"]),\n",
    "    (\"All_FU_Over_All_Total\", lambda d: F.when(F.col(\"All_Total\")!=0, (F.col(\"All_FU\")/F.col(\"All_Total\"))*100),\n",
    "     [\"All_FU\", \"All_Total\"]),\n",
    "    (\"All_First_Over_All_Total\", lambda d: F.when(F.col(\"All_Total\")!=0, (F.col(\"All_First\")/F.col(\"All_Total\"))*100),\n",
    "     [\"All_First\", \"All_Total\"]),\n",
    "    (\"All_NoProc_Over_All_Total\", lambda d: F.when(F.col(\"All_Total\")!=0, (F.col(\"All_NoProc\")/F.col(\"All_Total\"))*100),\n",
    "     [\"All_NoProc\", \"All_Total\"]),\n",
    "    (\"All_Proc_Over_All_Total\", lambda d: F.when(F.col(\"All_Total\")!=0, (F.col(\"All_Proc\")/F.col(\"All_Total\"))*100),\n",
    "     [\"All_Proc\", \"All_Total\"]),\n",
    "    (\"Remote_Total_Over_All_Total\", lambda d: F.when(F.col(\"All_Total\")!=0, (F.col(\"Remote_Total\")/F.col(\"All_Total\"))*100),\n",
    "     [\"Remote_Total\", \"All_Total\"]),\n",
    "    (\"Remote_FU_Over_All_FU\", lambda d: F.when(F.col(\"All_FU\")!=0, (F.col(\"Remote_FU\")/F.col(\"All_FU\"))*100),\n",
    "     [\"Remote_FU\", \"All_FU\"]),\n",
    "    (\"Remote_First_Over_All_First\", lambda d: F.when(F.col(\"All_First\")!=0, (F.col(\"Remote_First\")/F.col(\"All_First\"))*100),\n",
    "     [\"Remote_First\", \"All_First\"]),\n",
    "    (\"F2F_DNA_Over_F2F_Total\", lambda d: F.when((F.col(\"F2F_Total\")+F.col(\"F2F_DNA\"))!=0,\n",
    "                                                (F.col(\"F2F_DNA\")/(F.col(\"F2F_Total\")+F.col(\"F2F_DNA\")))*100),\n",
    "     [\"F2F_Total\", \"F2F_DNA\"]),\n",
    "    (\"Remote_DNA_Over_Remote_Total\", lambda d: F.when((F.col(\"Remote_Total\")+F.col(\"Remote_DNA\"))!=0,\n",
    "                                                      (F.col(\"Remote_DNA\")/(F.col(\"Remote_Total\")+F.col(\"Remote_DNA\")))*100),\n",
    "     [\"Remote_Total\", \"Remote_DNA\"]),\n",
    "]\n",
    "\n",
    "# Apply metrics dynamically\n",
    "for name, expr, req in metrics:\n",
    "    df = safe_add(df, name, expr, req)\n",
    "\n",
    "# Add simple duplicates & combinations (only if columns exist)\n",
    "simple_copies = [\n",
    "    (\"All_DNA1\", \"All_DNA\"),\n",
    "    (\"All_DNA2\", \"All_DNA\"),\n",
    "    (\"All_First1\", \"All_First\"),\n",
    "    (\"All_First2\", \"All_First\"),\n",
    "    (\"All_First3\", \"All_First\"),\n",
    "    (\"All_FU1\", \"All_FU\"),\n",
    "    (\"All_FU2\", \"All_FU\"),\n",
    "    (\"All_FU3\", \"All_FU\"),\n",
    "    (\"All_FU4\", \"All_FU\"),\n",
    "    (\"All_FU5\", \"All_FU\"),\n",
    "    (\"All_Total1\", \"All_Total\"),\n",
    "    (\"All_Total2\", \"All_Total\"),\n",
    "    (\"All_Total3\", \"All_Total\"),\n",
    "    (\"All_Total4\", \"All_Total\"),\n",
    "    (\"All_Total5\", \"All_Total\"),\n",
    "    (\"All_Total6\", \"All_Total\"),\n",
    "    (\"Remote_Total1\", \"Remote_Total\"),\n",
    "    (\"Remote_Total2\", \"Remote_Total\"),\n",
    "]\n",
    "for newc, base in simple_copies:\n",
    "    if base in df.columns:\n",
    "        df = df.withColumn(newc, F.col(base))\n",
    "    else:\n",
    "        df = df.withColumn(newc, F.lit(None))\n",
    "\n",
    "# Add a few combined sums (safe)\n",
    "combos = [\n",
    "    (\"All_First_plus_All_First_DNA\", [\"All_First\", \"All_First_DNA\"]),\n",
    "    (\"All_FU_plus_All_FU_DNA\", [\"All_FU\", \"All_FU_DNA\"]),\n",
    "    (\"All_Total_plus_All_DNA\", [\"All_Total\", \"All_DNA\"]),\n",
    "    (\"F2F_Total_plus_F2F_DNA\", [\"F2F_Total\", \"F2F_DNA\"]),\n",
    "    (\"Remote_Total_plus_Remote_DNA\", [\"Remote_Total\", \"Remote_DNA\"]),\n",
    "]\n",
    "for newc, cols in combos:\n",
    "    if all(c in df.columns for c in cols):\n",
    "        df = df.withColumn(newc, F.col(cols[0]).cast(\"long\") + F.col(cols[1]).cast(\"long\"))\n",
    "    else:\n",
    "        df = df.withColumn(newc, F.lit(None))\n",
    "\n",
    "# Final rename to match previous convention\n",
    "opa_final_with_added_metrics = df\n",
    "\n",
    "display(opa_final_with_added_metrics.limit(10))\n",
    "print(f\" Container 10 complete — {opa_final_with_added_metrics.count()} rows, {len(opa_final_with_added_metrics.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8a655039-435d-48d3-ae22-e6e831d4fb25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "**For step 10a**\n",
    "\n",
    "Click to expand for full details.\n",
    "\n",
    "This step introduces a benchmarking metric to support performance comparison across providers. It calculates the 25th percentile (lower quartile) of Remote_Total activity for each month and provider (Adj Org Code), representing a lower performance benchmark. The benchmark is then joined back to the main dataset and missing values are defaulted to zero. This provides a consistent reference point for analysing variation in remote activity levels across organisations and reporting periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acc65f15-e5a2-45db-9a01-2685810ed977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 10A – Add Remote Lower Benchmark\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Start from container 10 output\n",
    "df_benchmark = opa_final_with_added_metrics\n",
    "\n",
    "# Step 1: Calculate 25th percentile of Remote_Total by month and Adj Org Code\n",
    "remote_lower = (\n",
    "    df_benchmark\n",
    "    .groupBy(\"Der_Activity_Month_Date\", \"Adj Org Code\")\n",
    "    .agg(\n",
    "        F.expr(\"percentile_approx(Remote_Total, 0.25)\").alias(\"Remote_Lower_Benchmark\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 2: Join benchmark back to main dataset\n",
    "df_with_benchmark = df_benchmark.join(\n",
    "    remote_lower,\n",
    "    on=[\"Der_Activity_Month_Date\", \"Adj Org Code\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Step 3: For missing values, fill with 0\n",
    "df_with_benchmark = df_with_benchmark.fillna({\"Remote_Lower_Benchmark\": 0})\n",
    "\n",
    "# Final output\n",
    "opa_final_with_remote_benchmark = df_with_benchmark\n",
    "\n",
    "display(opa_final_with_remote_benchmark.limit(10))\n",
    "print(f\"Container 10A complete — {opa_final_with_remote_benchmark.count()} rows, {len(opa_final_with_remote_benchmark.columns)} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fe93a149-d55d-40ef-8536-55cb7fa074d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "**For step 11 summary**\n",
    "\n",
    "Click to expand for full details.\n",
    "\n",
    "This step reshapes the wide outpatient dataset into a long (tidy) format for easier analysis and visualization. Using PySparks explode and struct functions, all numeric metric columns are unpivoted into key value pairs (Metric_Name, Metric_Value) while retaining key identifiers like month, provider, ICB, and region. A combined metric label (Metric_Name_Treatment_Function_Group) is also generated to preserve clinical context. The final ordered table supports flexible reporting, dashboarding, and time-series comparisons across all activity metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c02eef9c-fc4e-4ee6-8f11-5f4cb9c538b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#11 reshapes the wide outpatient dataset into a long (tidy) format for easier analysis\n",
    "from pyspark.sql.functions import col, explode, array, struct, lit, concat_ws\n",
    "\n",
    "# ID columns to keep\n",
    "id_cols = [\n",
    "    \"Der_Activity_Month_Date\",\n",
    "    \"Der_Provider_Code\",\n",
    "    \"Treatment_Function_Group\",\n",
    "    \"Adj Org Code\",\n",
    "    \"ICB\",\n",
    "    \"Region_Code\"\n",
    "]\n",
    "\n",
    "# Identify all metric columns\n",
    "metric_cols = [c for c in opa_final_with_added_metrics.columns if c not in id_cols]\n",
    "\n",
    "# Unpivot numeric metrics\n",
    "opa_long = (\n",
    "    opa_final_with_added_metrics\n",
    "    .select(\n",
    "        *id_cols,\n",
    "        explode(array(*[\n",
    "            struct(lit(c).alias(\"Metric_Name\"), col(c).alias(\"Metric_Value\")) for c in metric_cols\n",
    "        ])).alias(\"kv\")\n",
    "    )\n",
    "    .select(\n",
    "        *id_cols,\n",
    "        col(\"kv.Metric_Name\"),\n",
    "        col(\"kv.Metric_Value\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the combined metric name\n",
    "opa_long = opa_long.withColumn(\n",
    "    \"Metric_Name_Treatment_Function_Group\",\n",
    "    concat_ws(\"_\", col(\"Metric_Name\"), col(\"Treatment_Function_Group\"))\n",
    ")\n",
    "\n",
    "# Order by date\n",
    "opa_long_ordered = opa_long.orderBy(\"Der_Activity_Month_Date\")\n",
    "\n",
    "display(opa_long_ordered.limit(10))\n",
    "print(f\"Number of rows in opa_long_ordered: {opa_long_ordered.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "264a39cb-b69c-4181-ae04-e5ef7cc5926b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "**For step 12**\n",
    "\n",
    "Click to expand for full details.\n",
    "\n",
    "This step consolidates the outpatient metrics into three hierarchical reporting levels: Organisation, ICB, and Region. Starting from the organisation-level dataset, core activity counts are summed across groups, and derived percentage metrics (e.g., DNA rates, procedure ratios, remote attendance rates) are recalculated for each level. The resulting dataframes are combined into a single unified dataset with a “Level” indicator and consistent organisation codes. This produces a complete, multi-level view of outpatient activity performance, ready for downstream reporting, benchmarking, and analytical use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bbdb2af-8682-41af-a49e-78981b93d63f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760606903925}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#12 – Aggregation and final metric derivation (Org, ICB, Region)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import when, col, lit\n",
    "\n",
    "# Start from Org-level counts from Container 9\n",
    "df_org = opa_final_with_added_metrics.withColumnRenamed(\"Adj Org Code\", \"Adj_Org_Code\")\n",
    "\n",
    "# Base metric columns to sum (removed \"F2F_DNA\")\n",
    "count_cols = [\n",
    "    \"All_Total\",\"All_First\",\"All_FU\",\"All_Proc\",\"All_NoProc\",\n",
    "    \"All_FU_Proc\",\"All_FU_NoProc\",\n",
    "    \"F2F_Total\",\"F2F_First\",\"F2F_FU\",\n",
    "    \"Remote_Total\",\"Remote_First\",\"Remote_FU\",\n",
    "    \"All_DNA\",\"All_First_DNA\",\"All_FU_DNA\",\n",
    "    \"Remote_DNA\",\n",
    "    \"All_2WW\",\"All_First_2WW\",\"All_FU_2WW\",\n",
    "    \"All_2WW_DNA\",\"All_First_2WW_DNA\",\"All_FU_2WW_DNA\"\n",
    "]\n",
    "\n",
    "# Function to (re)calculate rates & derived metrics (removed F2F_DNA_Over_F2F_Total)\n",
    "def add_rate_metrics(df):\n",
    "    return (\n",
    "        df\n",
    "        .withColumn(\"All_DNA_Over_All_Total\", F.when((F.col(\"All_Total\")+F.col(\"All_DNA\"))!=0,\n",
    "            (F.col(\"All_DNA\")/(F.col(\"All_Total\")+F.col(\"All_DNA\")))*100).otherwise(None))\n",
    "        .withColumn(\"All_First_DNA_Over_All_First\", F.when((F.col(\"All_First\")+F.col(\"All_First_DNA\"))!=0,\n",
    "            (F.col(\"All_First_DNA\")/(F.col(\"All_First\")+F.col(\"All_First_DNA\")))*100).otherwise(None))\n",
    "        .withColumn(\"All_FU_DNA_Over_All_FU\", F.when((F.col(\"All_FU\")+F.col(\"All_FU_DNA\"))!=0,\n",
    "            (F.col(\"All_FU_DNA\")/(F.col(\"All_FU\")+F.col(\"All_FU_DNA\")))*100).otherwise(None))\n",
    "        .withColumn(\"All_2WW_DNA_Over_All_2WW\", F.when(F.col(\"All_2WW\")!=0,\n",
    "            (F.col(\"All_2WW_DNA\")/F.col(\"All_2WW\"))*100).otherwise(None))\n",
    "        .withColumn(\"All_First_2WW_DNA_Over_All_First_2WW\", F.when(F.col(\"All_First_2WW\")!=0,\n",
    "            (F.col(\"All_First_2WW_DNA\")/F.col(\"All_First_2WW\"))*100).otherwise(None))\n",
    "        .withColumn(\"All_FU_2WW_DNA_Over_All_FU_2WW\", F.when(F.col(\"All_FU_2WW\")!=0,\n",
    "            (F.col(\"All_FU_2WW_DNA\")/F.col(\"All_FU_2WW\"))*100).otherwise(None))\n",
    "        .withColumn(\"All_FU_Over_All_Total\", F.when(F.col(\"All_Total\")!=0,\n",
    "            (F.col(\"All_FU\")/F.col(\"All_Total\"))*100).otherwise(None))\n",
    "        .withColumn(\"All_First_Over_All_Total\", F.when(F.col(\"All_Total\")!=0,\n",
    "            (F.col(\"All_First\")/F.col(\"All_Total\"))*100).otherwise(None))\n",
    "        .withColumn(\"All_NoProc_Over_All_Total\", F.when(F.col(\"All_Total\")!=0,\n",
    "            (F.col(\"All_NoProc\")/F.col(\"All_Total\"))*100).otherwise(None))\n",
    "        .withColumn(\"All_Proc_Over_All_Total\", F.when(F.col(\"All_Total\")!=0,\n",
    "            (F.col(\"All_Proc\")/F.col(\"All_Total\"))*100).otherwise(None))\n",
    "        .withColumn(\"Remote_Total_Over_All_Total\", F.when(F.col(\"All_Total\")!=0,\n",
    "            (F.col(\"Remote_Total\")/F.col(\"All_Total\"))*100).otherwise(None))\n",
    "        .withColumn(\"Remote_FU_Over_All_FU\", F.when(F.col(\"All_FU\")!=0,\n",
    "            (F.col(\"Remote_FU\")/F.col(\"All_FU\"))*100).otherwise(None))\n",
    "        .withColumn(\"Remote_First_Over_All_First\", F.when(F.col(\"All_First\")!=0,\n",
    "            (F.col(\"Remote_First\")/F.col(\"All_First\"))*100).otherwise(None))\n",
    "        .withColumn(\"Remote_DNA_Over_Remote_Total\", F.when((F.col(\"Remote_Total\")+F.col(\"Remote_DNA\"))!=0,\n",
    "            (F.col(\"Remote_DNA\")/(F.col(\"Remote_Total\")+F.col(\"Remote_DNA\")))*100).otherwise(None))\n",
    "    )\n",
    "\n",
    "# Aggregate to ICB\n",
    "df_icb = (\n",
    "    df_org.groupBy(\"Der_Activity_Month_Date\", \"ICB\", \"Treatment_Function_Group\")\n",
    "    .agg(*[F.sum(F.col(c)).alias(c) for c in count_cols])\n",
    ")\n",
    "df_icb = add_rate_metrics(df_icb).withColumn(\"Level\", F.lit(\"ICB\"))\n",
    "\n",
    "# Aggregate to Region\n",
    "df_region = (\n",
    "    df_org.groupBy(\"Der_Activity_Month_Date\", \"Region_Code\", \"Treatment_Function_Group\")\n",
    "    .agg(*[F.sum(F.col(c)).alias(c) for c in count_cols])\n",
    ")\n",
    "df_region = add_rate_metrics(df_region).withColumn(\"Level\", F.lit(\"Region\"))\n",
    "\n",
    "# Label Org-level rows\n",
    "df_org = df_org.withColumn(\"Level\", F.lit(\"Org\"))\n",
    "\n",
    "# Combine all levels into one dataset\n",
    "final_output = (\n",
    "    df_org.unionByName(df_icb, allowMissingColumns=True)\n",
    "           .unionByName(df_region, allowMissingColumns=True)\n",
    ")\n",
    "\n",
    "# Adjust codes based on Level\n",
    "final_output = final_output.withColumn(\n",
    "    \"Adj_Org_Code_Final\",\n",
    "    when(col(\"Level\") == \"Org\", col(\"Adj_Org_Code\"))\n",
    "    .when(col(\"Level\") == \"ICB\", col(\"ICB\"))\n",
    "    .when(col(\"Level\") == \"Region\", col(\"Region_Code\"))\n",
    ")\n",
    "\n",
    "# Save and display\n",
    "final_output.write.mode(\"overwrite\").parquet(\"/mnt/output/opa_final_all_levels\")\n",
    "\n",
    "display(final_output.limit(10))\n",
    "print(f\"Rows in final output: {final_output.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "37e6447b-e187-4dc2-a95e-47cc489117f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "**Step 13**\n",
    "\n",
    "Click to expand for full details.\n",
    "\n",
    "This step reshapes the multi-level outpatient dataset into a long (skinny) OPRT format, suitable for reporting or operational analytics. Using PySparks explode and struct, all numeric metrics are unpivoted into key value pairs (OPRT_Metric_Name, Metric_Value), while retaining identifiers such as month, region, ICB, organisation, and treatment function group. The resulting table provides a tidy, standardised structure for downstream processing, enabling easy filtering, aggregation, and integration with reporting tools or performance dashboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc8aa02e-dd35-4b70-a01f-17d8baa7486c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#13 – Convert to long/skinny OPRT format\n",
    "from pyspark.sql.functions import col, lit, explode, array, struct\n",
    "\n",
    "# Use the final_output table from container 12\n",
    "df_wide = final_output\n",
    "\n",
    "# Define identifier columns\n",
    "id_cols = [\n",
    "    \"Der_Activity_Month_Date\",\n",
    "    \"Region_Code\",\n",
    "    \"ICB\",\n",
    "    \"Adj_Org_Code_Final\",\n",
    "    \"Treatment_Function_Group\"\n",
    "]\n",
    "\n",
    "# Identify metric columns (exclude identifiers and helper columns)\n",
    "metric_cols = [c for c in df_wide.columns if c not in id_cols + [\"Level\", \"Adj_Org_Code\"]]\n",
    "\n",
    "# Melt into long format\n",
    "opa_oprt_long = (\n",
    "    df_wide.select(\n",
    "        *id_cols,\n",
    "        explode(array(*[\n",
    "            struct(lit(c).alias(\"OPRT_Metric_Name\"), col(c).alias(\"Metric_Value\"))\n",
    "            for c in metric_cols\n",
    "        ])).alias(\"kv\")\n",
    "    )\n",
    "    .select(\n",
    "        *id_cols,\n",
    "        col(\"kv.OPRT_Metric_Name\"),\n",
    "        col(\"kv.Metric_Value\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Rename column for clarity\n",
    "opa_oprt_long = opa_oprt_long.withColumnRenamed(\"Treatment_Function_Group\", \"Treatment_Function_Group\")\n",
    "\n",
    "display(opa_oprt_long.limit(10))\n",
    "print(f\"Container 13 complete — {opa_oprt_long.count()} rows, {len(opa_oprt_long.columns)} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "147a17aa-a1e8-4cc2-8039-a6ab55236a72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "**Step 14**\n",
    "\n",
    "Click to expand for full details.\n",
    "\n",
    "This final step enhances the long-format outpatient dataset by creating a combined metric identifier (OPRT_Metric_Name_TFC) that merges each metric with its Treatment Function Group. The dataset is then joined with mhs_metric_list to assign a corresponding InternalID for consistent internal tracking and reporting. The resulting table is written to Parquet, providing a fully standardised, identifier-enriched dataset ready for operational reporting, benchmarking, and downstream analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a17e7bc2-c4e7-41b4-a017-b261ac96fc62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#14 – Add OPRT Metric Name_TFC and join to mhs_metric_list for InternalID\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import concat_ws, col\n",
    "\n",
    "# Start from container 13 output\n",
    "df_metrics = opa_oprt_long\n",
    "\n",
    "# Step 1: Create combined OPRT metric name and TFC\n",
    "df_metrics = df_metrics.withColumn(\n",
    "    \"OPRT_Metric_Name_TFC\",\n",
    "    concat_ws(\"_\", col(\"OPRT_Metric_Name\"), col(\"Treatment_Function_Group\"))\n",
    ")\n",
    "\n",
    "# Step 2: Join with mhs_metric_list on combined name only\n",
    "df_with_id = df_metrics.join(\n",
    "    mhs_metric_list.select(\n",
    "        F.col(\"OPRT Metric Name _TFC\").alias(\"join_metric\"),\n",
    "        F.col(\"InternalID\")\n",
    "    ),\n",
    "    df_metrics[\"OPRT_Metric_Name_TFC\"] == F.col(\"join_metric\"),\n",
    "    \"left\"\n",
    ").drop(\"join_metric\")\n",
    "\n",
    "# Step 3: Write and show final table\n",
    "df_with_id.write.mode(\"overwrite\").parquet(\"/mnt/output/opa_oprt_final\")\n",
    "\n",
    "display(df_with_id.limit(10))\n",
    "print(f\"Container 14 complete — {df_with_id.count()} rows, {len(df_with_id.columns)} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1125d19-3066-4171-b99c-95955e2286a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#15 Saving the file to the lake mart for QA\n",
    "df_with_id.coalesce(1).write \\\n",
    "    .format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/Projects/OP_QA_STP_Region_new_metrics_v9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "156c85e8-7836-4ddc-8b9b-c39c915e40df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#16 Saving the file to the lake mart for QA (filtered for a small sample)\n",
    "df_with_id.filter(\n",
    "    (F.col(\"Der_Activity_Month_Date\") == \"2025-07-31\") & \n",
    "    (F.col(\"Adj_Org_Code_Final\") == \"RH8\")\n",
    ").coalesce(1).write \\\n",
    "    .format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/Projects/OP_QA_STP_Region_new_metrics_short_vi.csv\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "5 Re-shaping the data v2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
