{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3069d1c2-d996-422e-9bb1-cc35dedef6c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1 Importing Tools \n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "from openpyxl.styles import NamedStyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fc08c57-eb66-4838-8238-ae0b006f0ce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2 Reduce risk of a timeout by increasing limit to 30 minutes\n",
    "spark.conf.set(\"spark.databricks.execution.timeout\", \"1800\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d21a9981-c2a1-46a2-9e9b-419c78eea302",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3 Loading the master hierarchies table from the lake mart\n",
    "df_master_hierarchies = spark.read.option(\"header\", \"true\").csv(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/EROC_Collection_Queries/master_hierarchies_table.csv\")\n",
    "#display(df_master_hierarchies.limit(10))\n",
    "#print(f\"Number of rows in master hierarchies: {df_master_hierarchies.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6334b52-eb66-4ef5-a95c-afea7c3e7ea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#4 loading ICB to Region table\n",
    "df_icb_region = spark.read.option(\"header\", \"true\").csv(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/EROC_Collection/EROC/EROC_ICB_Region_DisplayNames.csv\")  # Ensure proper Azure credentials are configured for ADLS access.\n",
    "#display(df_icb_region.limit(10))\n",
    "#print(f\"Number of rows in icb_region: {df_icb_region.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a9b0856-5c30-47d8-8e98-a08bab8926f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#5 loading list of merged providers\n",
    "df_merged_providers = spark.read.option(\"header\", \"true\").csv(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/EROC_Collection/EROC/EROC_Merged_Providers.csv\")\n",
    "#display(df_merged_providers.limit(10))\n",
    "#print(f\"Number of rows in merged providers: {df_merged_providers.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a08aeef7-8c39-402c-be16-83f587b7c999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#6 creating new provider code from the provider mapping table\n",
    "provider_code_mapping = df_merged_providers = spark.read.option(\"header\", \"true\").csv(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/EROC_Collection/EROC/EROC_Merged_Providers.csv\")\n",
    "#display(df_merged_providers.limit(10))\n",
    "#print(f\"Number of rows in merged providers: {df_merged_providers.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "373bf4a4-013f-42f6-81eb-76f55d979cd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#7a importing MHS metric list and internal ID\n",
    "mhs_metric_list = spark.read.option(\"header\", \"true\").csv(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/EROC_Collection/MHS\")\n",
    "#display(mhs_metric_list.limit(10))\n",
    "#print(f\"Number of rows in mhs_metric_list: {mhs_metric_list.count()}\")\n",
    "\n",
    "#7b importing MHS allowable org codes\n",
    "mhs_allowable_orgs = spark.read.option(\"header\", \"true\").csv(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/EROC_Collection/MHS/Allowable_Org_Codes_Status.csv\")\n",
    "#display(mhs_allowable_orgs.limit(10))\n",
    "#print(f\"Number of rows in mhs_allowable_orgs: {mhs_allowable_orgs.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c2a5552-3d8d-4900-8371-c80952cf4b03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#8 Loading the core monthly snapshot data\n",
    "from pyspark.sql import functions as F\n",
    "df_op_activity_snapshot = spark.read.option(\"header\", \"true\").option(\"recursiveFileLookup\", \"true\").parquet(\n",
    "    \"abfss://reporting@udalstdatacuratedprod.dfs.core.windows.net/restricted/patientlevel/MESH/OPA/OPA_Core_Monthly_Snapshot/Published/1\"\n",
    ")\n",
    "#display(df_op_activity_snapshot.limit(10))\n",
    "\n",
    "# Show number of rows in the raw data\n",
    "row_count = df_op_activity_snapshot.count()\n",
    "print(f\"Number of rows in raw data: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f82af93-37d4-4281-b468-6e017347849f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#9 Creating the wide table & inserting new column for merged providers with new merger codes and mapping to ICB and Region codes\n",
    "from pyspark.sql.functions import when, col, lit, create_map, coalesce, last_day, to_date, concat_ws\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Define valid treatment function codes\n",
    "VALID_TREATMENT_CODES = [\n",
    "    '100', '101', '102', '104', '105', '106', '108', '110', '111', '115', '120', '130', '140',\n",
    "    '144', '145', '301', '302', '303', '307', '320', '330', '340', '361', '400', '410', '420',\n",
    "    '430', '501', '502', '560', '650'\n",
    "]\n",
    "\n",
    "# Adding in the Treatment_Function_Code_New column\n",
    "opa_with_tfc = df_op_activity_snapshot.withColumn(\n",
    "    \"Treatment_Function_Code_New\",\n",
    "    when(col(\"Treatment_Function_Code\").isin(VALID_TREATMENT_CODES), col(\"Treatment_Function_Code\")).otherwise(\"Other\")\n",
    ")\n",
    "\n",
    "# Add Treatment_Function_Group column using VALID_TREATMENT_CODES groupings\n",
    "opa_with_groups = opa_with_tfc.withColumn(\n",
    "    \"Treatment_Function_Group\",\n",
    "    when(col(\"Treatment_Function_Code_New\").isin(\"100\", \"102\", \"104\", \"105\", \"106\"), \"GS\")\n",
    "     .when(col(\"Treatment_Function_Code_New\").isin(\"140\", \"144\", \"145\"), \"OMFS\")\n",
    "     .when(col(\"Treatment_Function_Code_New\").isin(\"110\", \"111\", \"115\"), \"T&O\")\n",
    "     .otherwise(col(\"Treatment_Function_Code_New\"))\n",
    ")\n",
    "\n",
    "# Filter dataset for relevant years, admin category, TFC, and attendance\n",
    "opa_filtered = opa_with_groups.filter(\n",
    "    (col(\"Der_Financial_Year\").isin(\"2023/24\", \"2024/25\", \"2025/26\")) &  # Update manually if needed\n",
    "    (col(\"Administrative_Category\") == \"01\") &\n",
    "    (col(\"Treatment_Function_Code\") != \"812\") &\n",
    "    (col(\"First_Attendance\").isin(\"1\", \"2\", \"3\", \"4\"))\n",
    ")\n",
    "\n",
    "# Aggregates the metrics by month, provider, and Treatment_Function_Group\n",
    "opa_agg = opa_filtered.groupBy(\n",
    "    \"Der_Activity_Month\",\n",
    "    \"Der_Provider_Code\",\n",
    "    \"Treatment_Function_Group\"\n",
    ").agg(\n",
    "    # All contacts\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"1\", \"2\", \"3\", \"4\")), 1).otherwise(0)).alias(\"All_Total\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"1\", \"3\")), 1).otherwise(0)).alias(\"All_First\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"2\", \"4\")), 1).otherwise(0)).alias(\"All_FU\"),\n",
    "    F.sum(when((col(\"Der_Number_Procedure\") > 0) & (col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"1\", \"2\", \"3\", \"4\")), 1).otherwise(0)).alias(\"All_Proc\"),\n",
    "    F.sum(when((col(\"Der_Number_Procedure\") == 0) & (col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"1\", \"2\", \"3\", \"4\")), 1).otherwise(0)).alias(\"All_NoProc\"),\n",
    "    F.sum(when((col(\"Der_Number_Procedure\") > 0) & (col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"2\", \"4\")), 1).otherwise(0)).alias(\"All_FU_Proc\"),\n",
    "    F.sum(when((col(\"Der_Number_Procedure\") == 0) & (col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"2\", \"4\")), 1).otherwise(0)).alias(\"All_FU_NoProc\"),\n",
    "    # Face-to-face\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"1\", \"2\")), 1).otherwise(0)).alias(\"F2F_Total\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\") == \"1\"), 1).otherwise(0)).alias(\"F2F_First\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\") == \"2\"), 1).otherwise(0)).alias(\"F2F_FU\"),\n",
    "    # Remote\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\").isin(\"3\", \"4\")), 1).otherwise(0)).alias(\"Remote_Total\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\") == \"3\"), 1).otherwise(0)).alias(\"Remote_First\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\")) & (col(\"First_Attendance\") == \"4\"), 1).otherwise(0)).alias(\"Remote_FU\"),\n",
    "    # Did not attends (DNAs)\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"3\", \"7\")) & (col(\"First_Attendance\").isin(\"1\", \"2\", \"3\", \"4\")), 1).otherwise(0)).alias(\"All_DNA\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"3\", \"7\")) & (col(\"First_Attendance\").isin(\"1\", \"3\")), 1).otherwise(0)).alias(\"All_First_DNA\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"3\", \"7\")) & (col(\"First_Attendance\").isin(\"2\", \"4\")), 1).otherwise(0)).alias(\"All_FU_DNA\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"3\", \"7\")) & (col(\"First_Attendance\").isin(\"1\", \"2\")), 1).otherwise(0)).alias(\"F2F_DNA\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"3\", \"7\")) & (col(\"First_Attendance\").isin(\"3\", \"4\")), 1).otherwise(0)).alias(\"Remote_DNA\"),\n",
    "    # 2WW DNA\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"3\", \"7\")) & (col(\"First_Attendance\").isin(\"1\", \"2\", \"3\", \"4\")) & (col(\"Priority_Type\") == \"3\"), 1).otherwise(0)).alias(\"All_2WW_DNA\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"3\", \"7\")) & (col(\"First_Attendance\").isin(\"1\", \"3\")) & (col(\"Priority_Type\") == \"3\"), 1).otherwise(0)).alias(\"All_First_2WW_DNA\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"3\", \"7\")) & (col(\"First_Attendance\").isin(\"2\", \"4\")) & (col(\"Priority_Type\") == \"3\"), 1).otherwise(0)).alias(\"All_FU_2WW_DNA\"),\n",
    "    # All 2WW appointments\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\", \"3\", \"7\")) & (col(\"First_Attendance\").isin(\"1\", \"2\", \"3\", \"4\")) & (col(\"Priority_Type\") == \"3\"), 1).otherwise(0)).alias(\"All_2WW\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\", \"3\", \"7\")) & (col(\"First_Attendance\").isin(\"1\", \"3\")) & (col(\"Priority_Type\") == \"3\"), 1).otherwise(0)).alias(\"All_First_2WW\"),\n",
    "    F.sum(when((col(\"Attendance_Status\").isin(\"5\", \"6\", \"3\", \"7\")) & (col(\"First_Attendance\").isin(\"2\", \"4\")) & (col(\"Priority_Type\") == \"3\"), 1).otherwise(0)).alias(\"All_FU_2WW\")\n",
    ")\n",
    "\n",
    "# Add \"All\" TFC totals by month and provider\n",
    "METRIC_COLS = [c for c in opa_agg.columns if c not in [\"Der_Activity_Month\", \"Der_Provider_Code\", \"Treatment_Function_Group\"]]\n",
    "\n",
    "opa_all_tfc = opa_agg.groupBy(\"Der_Activity_Month\", \"Der_Provider_Code\").agg(\n",
    "    *[F.sum(col(c)).alias(c) for c in METRIC_COLS]\n",
    ").withColumn(\"Treatment_Function_Group\", lit(\"All\"))\n",
    "\n",
    "opa_final = opa_agg.unionByName(opa_all_tfc)\n",
    "\n",
    "# Order by results\n",
    "opa_final_ordered = opa_final.orderBy(\"Der_Activity_Month\", \"Der_Provider_Code\", \"Treatment_Function_Group\")\n",
    "\n",
    "# Inserted mapping code to build mapping_expr from df_merged_providers\n",
    "provider_code_mapping_dict = {\n",
    "    row['Old_Provider_Code']: row['New_Provider_Code']\n",
    "    for row in df_merged_providers.select(\"Old_Provider_Code\", \"New_Provider_Code\").distinct().collect()\n",
    "}\n",
    "\n",
    "mapping_list = []\n",
    "for k, v in provider_code_mapping_dict.items():\n",
    "    mapping_list.append(lit(k))\n",
    "    mapping_list.append(lit(v))\n",
    "\n",
    "mapping_expr = create_map(mapping_list)\n",
    "\n",
    "# Add \"Adj Org Code\" column based on provider_code_mapping\n",
    "opa_final_ordered_with_adj = opa_final_ordered.withColumn(\n",
    "    \"Adj Org Code\",\n",
    "    coalesce(mapping_expr.getItem(col(\"Der_Provider_Code\")), col(\"Der_Provider_Code\"))\n",
    ")\n",
    "\n",
    "# Add \"ICB\" column by joining to df_master_hierarchies on Organisation_Code and returning STP_Code (ICB)\n",
    "opa_final_ordered_with_icb = opa_final_ordered_with_adj.join(\n",
    "    df_master_hierarchies.select(\n",
    "        F.col(\"Organisation_Code\").alias(\"join_org_code\"),\n",
    "        F.col(\"STP_Code\").alias(\"ICB\")\n",
    "    ),\n",
    "    opa_final_ordered_with_adj[\"Adj Org Code\"] == F.col(\"join_org_code\"),\n",
    "    \"left\"\n",
    ").drop(\"join_org_code\")\n",
    "\n",
    "# Add \"Region\" column by joining to df_icb_region on ICB column and returning Region_Code\n",
    "opa_final_ordered_with_icb_region = opa_final_ordered_with_icb.join(\n",
    "    df_icb_region.select(\n",
    "        F.col(\"ICB_Code\").alias(\"join_icb\"),\n",
    "        F.col(\"Region_Code\")\n",
    "    ),\n",
    "    opa_final_ordered_with_icb[\"ICB\"] == F.col(\"join_icb\"),\n",
    "    \"left\"\n",
    ").drop(\"join_icb\")\n",
    "\n",
    "# Build Der_Activity_Month_Date from Der_Activity_Month (YYYYMM -> month end date)\n",
    "opa_final_ordered_with_icb_region = opa_final_ordered_with_icb_region.withColumn(\n",
    "    \"Der_Activity_Month_Date\",\n",
    "    last_day(\n",
    "        to_date(\n",
    "            concat_ws(\n",
    "                '-',\n",
    "                col(\"Der_Activity_Month\").substr(1, 4),\n",
    "                col(\"Der_Activity_Month\").substr(5, 2),\n",
    "                lit(\"01\")\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Count for debugging if needed\n",
    "opa_final_ordered_with_icb_region_row_count = opa_final_ordered_with_icb_region.count()\n",
    "\n",
    "# Drop unwanted columns, aggregate metrics, and sort the final table\n",
    "id_cols = [\"Der_Activity_Month_Date\", \"Treatment_Function_Group\", \"Region_Code\", \"ICB\", \"Adj Org Code\"]\n",
    "\n",
    "# Determine metric columns (exclude identifiers and the three columns to drop)\n",
    "metric_cols = [\n",
    "    c for c in opa_final_ordered_with_icb_region.columns\n",
    "    if c not in id_cols + [\"Der_Activity_Month\", \"Der_Provider_Code\", \"Treatment_Function_Code_New\"]\n",
    "]\n",
    "\n",
    "opa_final_processed = (\n",
    "    opa_final_ordered_with_icb_region\n",
    "    .groupBy(*[F.col(c) for c in id_cols])\n",
    "    .agg(*[F.sum(F.col(c)).alias(c) for c in metric_cols])\n",
    "    .orderBy(\"Der_Activity_Month_Date\", \"Region_Code\", \"ICB\", \"Adj Org Code\", \"Treatment_Function_Group\")\n",
    ")\n",
    "\n",
    "# display(opa_final_processed.limit(10))\n",
    "# print(f\"Number of rows in opa_final_processed: {opa_final_processed.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab1177ed-00ad-44f3-8eda-776b6093dd07",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760706170841}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#10 — Safe metric calculation (robust to missing columns)\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = opa_final_ordered_with_icb_region\n",
    "\n",
    "def safe_add(df, new_col, expr_fn, required_cols):\n",
    "    if all(c in df.columns for c in required_cols):\n",
    "        return df.withColumn(new_col, expr_fn(df))\n",
    "    else:\n",
    "        return df.withColumn(new_col, F.lit(None))\n",
    "\n",
    "metrics = [\n",
    "    (\"All_DNA_Over_All_Total\", lambda d: F.when(\n",
    "        (F.col(\"All_Total\") + F.col(\"All_DNA\")) != 0,\n",
    "        (F.col(\"All_DNA\") / (F.col(\"All_Total\") + F.col(\"All_DNA\"))) * 100\n",
    "    ), [\"All_Total\", \"All_DNA\"]),\n",
    "    (\"All_DNA_Over_All_Total_IG\", lambda d: F.when(\n",
    "        (F.col(\"All_Total\") + F.col(\"All_DNA\")) != 0,\n",
    "        (F.col(\"All_DNA\") / (F.col(\"All_Total\") + F.col(\"All_DNA\"))) * 100\n",
    "    ), [\"All_Total\", \"All_DNA\"]),\n",
    "    (\"All_First_DNA_Over_All_First\", lambda d: F.when(\n",
    "        (F.col(\"All_First\") + F.col(\"All_First_DNA\")) != 0,\n",
    "        (F.col(\"All_First_DNA\") / (F.col(\"All_First\") + F.col(\"All_First_DNA\"))) * 100\n",
    "    ), [\"All_First\", \"All_First_DNA\"]),\n",
    "    (\"All_First_DNA_Over_All_First_IG\", lambda d: F.when(\n",
    "        (F.col(\"All_First\") + F.col(\"All_First_DNA\")) != 0,\n",
    "        (F.col(\"All_First_DNA\") / (F.col(\"All_First\") + F.col(\"All_First_DNA\"))) * 100\n",
    "    ), [\"All_First\", \"All_First_DNA\"]),\n",
    "    (\"All_FU_DNA_Over_All_FU\", lambda d: F.when(\n",
    "        (F.col(\"All_FU\") + F.col(\"All_FU_DNA\")) != 0,\n",
    "        (F.col(\"All_FU_DNA\") / (F.col(\"All_FU\") + F.col(\"All_FU_DNA\"))) * 100\n",
    "    ), [\"All_FU\", \"All_FU_DNA\"]),\n",
    "    (\"All_FU_DNA_Over_All_FU_IG\", lambda d: F.when(\n",
    "        (F.col(\"All_FU\") + F.col(\"All_FU_DNA\")) != 0,\n",
    "        (F.col(\"All_FU_DNA\") / (F.col(\"All_FU\") + F.col(\"All_FU_DNA\"))) * 100\n",
    "    ), [\"All_FU\", \"All_FU_DNA\"]),\n",
    "    (\"All_2WW_DNA_Over_All_2WW\", lambda d: F.when(\n",
    "        (F.col(\"All_2WW\") != 0) & (F.col(\"All_2WW\").isNotNull()),\n",
    "        (F.col(\"All_2WW_DNA\") / F.col(\"All_2WW\")) * 100\n",
    "    ), [\"All_2WW_DNA\", \"All_2WW\"]),\n",
    "    (\"All_FU_2WW_DNA_Over_All_FU_2WW\", lambda d: F.when(\n",
    "        (F.col(\"All_FU_2WW\") != 0) & (F.col(\"All_FU_2WW\").isNotNull()),\n",
    "        (F.col(\"All_FU_2WW_DNA\") / F.col(\"All_FU_2WW\")) * 100\n",
    "    ), [\"All_FU_2WW_DNA\", \"All_FU_2WW\"]),\n",
    "    (\"All_First_2WW_DNA_Over_All_First_2WW\", lambda d: F.when(\n",
    "        (F.col(\"All_First_2WW\") != 0) & (F.col(\"All_First_2WW\").isNotNull()),\n",
    "        (F.col(\"All_First_2WW_DNA\") / F.col(\"All_First_2WW\")) * 100\n",
    "    ), [\"All_First_2WW_DNA\", \"All_First_2WW\"]),\n",
    "    (\"All_FU_NoProc_Over_All_FU\", lambda d: F.when(\n",
    "        F.col(\"All_FU\") != 0, (F.col(\"All_FU_NoProc\") / F.col(\"All_FU\")) * 100\n",
    "    ), [\"All_FU_NoProc\", \"All_FU\"]),\n",
    "    (\"All_FU_Proc_Over_All_FU\", lambda d: F.when(\n",
    "        F.col(\"All_FU\") != 0, (F.col(\"All_FU_Proc\") / F.col(\"All_FU\")) * 100\n",
    "    ), [\"All_FU_Proc\", \"All_FU\"]),\n",
    "    (\"All_FU_To_All_First\", lambda d: F.when(\n",
    "        F.col(\"All_First\") != 0, (F.col(\"All_FU\") / F.col(\"All_First\"))\n",
    "    ), [\"All_FU\", \"All_First\"]),\n",
    "    (\"All_FU_Over_All_Total\", lambda d: F.when(\n",
    "        F.col(\"All_Total\") != 0, (F.col(\"All_FU\") / F.col(\"All_Total\")) * 100\n",
    "    ), [\"All_FU\", \"All_Total\"]),\n",
    "    (\"All_First_Over_All_Total\", lambda d: F.when(\n",
    "        F.col(\"All_Total\") != 0, (F.col(\"All_First\") / F.col(\"All_Total\")) * 100\n",
    "    ), [\"All_First\", \"All_Total\"]),\n",
    "    (\"All_NoProc_Over_All_Total\", lambda d: F.when(\n",
    "        F.col(\"All_Total\") != 0, (F.col(\"All_NoProc\") / F.col(\"All_Total\")) * 100\n",
    "    ), [\"All_NoProc\", \"All_Total\"]),\n",
    "    (\"All_Proc_Over_All_Total\", lambda d: F.when(\n",
    "        F.col(\"All_Total\") != 0, (F.col(\"All_Proc\") / F.col(\"All_Total\")) * 100\n",
    "    ), [\"All_Proc\", \"All_Total\"]),\n",
    "    (\"Remote_Total_Over_All_Total\", lambda d: F.when(\n",
    "        F.col(\"All_Total\") != 0, (F.col(\"Remote_Total\") / F.col(\"All_Total\")) * 100\n",
    "    ), [\"Remote_Total\", \"All_Total\"]),\n",
    "    (\"Remote_FU_Over_All_FU\", lambda d: F.when(\n",
    "        F.col(\"All_FU\") != 0, (F.col(\"Remote_FU\") / F.col(\"All_FU\")) * 100\n",
    "    ), [\"Remote_FU\", \"All_FU\"]),\n",
    "    (\"Remote_First_Over_All_First\", lambda d: F.when(\n",
    "        F.col(\"All_First\") != 0, (F.col(\"Remote_First\") / F.col(\"All_First\")) * 100\n",
    "    ), [\"Remote_First\", \"All_First\"]),\n",
    "    (\"F2F_DNA_Over_F2F_Total\", lambda d: F.when(\n",
    "        (F.col(\"F2F_Total\") + F.col(\"F2F_DNA\")) != 0,\n",
    "        (F.col(\"F2F_DNA\") / (F.col(\"F2F_Total\") + F.col(\"F2F_DNA\"))) * 100\n",
    "    ), [\"F2F_Total\", \"F2F_DNA\"]),\n",
    "    (\"Remote_DNA_Over_Remote_Total\", lambda d: F.when(\n",
    "        (F.col(\"Remote_Total\") + F.col(\"Remote_DNA\")) != 0,\n",
    "        (F.col(\"Remote_DNA\") / (F.col(\"Remote_Total\") + F.col(\"Remote_DNA\"))) * 100\n",
    "    ), [\"Remote_Total\", \"Remote_DNA\"]),\n",
    "]\n",
    "\n",
    "for name, expr, req in metrics:\n",
    "    df = safe_add(df, name, expr, req)\n",
    "\n",
    "simple_copies = [\n",
    "    (\"All_DNA1\", \"All_DNA\"),\n",
    "    (\"All_DNA2\", \"All_DNA\"),\n",
    "    (\"All_First1\", \"All_First\"),\n",
    "    (\"All_First2\", \"All_First\"),\n",
    "    (\"All_First3\", \"All_First\"),\n",
    "    (\"All_FU1\", \"All_FU\"),\n",
    "    (\"All_FU2\", \"All_FU\"),\n",
    "    (\"All_FU3\", \"All_FU\"),\n",
    "    (\"All_FU4\", \"All_FU\"),\n",
    "    (\"All_FU5\", \"All_FU\"),\n",
    "    (\"All_Total1\", \"All_Total\"),\n",
    "    (\"All_Total2\", \"All_Total\"),\n",
    "    (\"All_Total3\", \"All_Total\"),\n",
    "    (\"All_Total4\", \"All_Total\"),\n",
    "    (\"All_Total5\", \"All_Total\"),\n",
    "    (\"All_Total6\", \"All_Total\"),\n",
    "    (\"Remote_Total1\", \"Remote_Total\"),\n",
    "    (\"Remote_Total2\", \"Remote_Total\"),\n",
    "]\n",
    "for newc, base in simple_copies:\n",
    "    if base in df.columns:\n",
    "        df = df.withColumn(newc, F.col(base))\n",
    "    else:\n",
    "        df = df.withColumn(newc, F.lit(None))\n",
    "\n",
    "combos = [\n",
    "    (\"All_First_plus_All_First_DNA\", [\"All_First\", \"All_First_DNA\"]),\n",
    "    (\"All_FU_plus_All_FU_DNA\", [\"All_FU\", \"All_FU_DNA\"]),\n",
    "    (\"All_Total_plus_All_DNA\", [\"All_Total\", \"All_DNA\"]),\n",
    "    (\"F2F_Total_plus_F2F_DNA\", [\"F2F_Total\", \"F2F_DNA\"]),\n",
    "    (\"Remote_Total_plus_Remote_DNA\", [\"Remote_Total\", \"Remote_DNA\"]),\n",
    "]\n",
    "for newc, cols in combos:\n",
    "    if all(c in df.columns for c in cols):\n",
    "        df = df.withColumn(newc, F.col(cols[0]).cast(\"long\") + F.col(cols[1]).cast(\"long\"))\n",
    "    else:\n",
    "        df = df.withColumn(newc, F.lit(None))\n",
    "\n",
    "cols_to_drop = [c for c in [\"Der_Activity_Month\", \"Der_Provider_Code\"] if c in df.columns]\n",
    "opa_final_with_added_metrics = df.drop(*cols_to_drop)\n",
    "\n",
    "display(opa_final_with_added_metrics.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c02eef9c-fc4e-4ee6-8f11-5f4cb9c538b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#11 reshapes the wide outpatient dataset into a long (tidy) format for easier analysis\n",
    "from pyspark.sql.functions import col, explode, array, struct, lit, concat_ws\n",
    "\n",
    "# ID columns to keep\n",
    "id_cols = [\n",
    "    \"Der_Activity_Month_Date\",\n",
    "    # \"Der_Provider_Code\",\n",
    "    \"Treatment_Function_Group\",\n",
    "    \"Adj Org Code\",\n",
    "    \"ICB\",\n",
    "    \"Region_Code\"\n",
    "]\n",
    "\n",
    "# Identify all metric columns\n",
    "metric_cols = [c for c in opa_final_with_added_metrics.columns if c not in id_cols]\n",
    "\n",
    "# Unpivot numeric metrics\n",
    "opa_long = (\n",
    "    opa_final_with_added_metrics\n",
    "    .select(\n",
    "        *id_cols,\n",
    "        explode(array(*[\n",
    "            struct(lit(c).alias(\"Metric_Name\"), col(c).alias(\"Metric_Value\")) for c in metric_cols\n",
    "        ])).alias(\"kv\")\n",
    "    )\n",
    "    .select(\n",
    "        *id_cols,\n",
    "        col(\"kv.Metric_Name\"),\n",
    "        col(\"kv.Metric_Value\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the combined metric name\n",
    "opa_long = opa_long.withColumn(\n",
    "    \"Metric_Name_Treatment_Function_Group\",\n",
    "    concat_ws(\"_\", col(\"Metric_Name\"), col(\"Treatment_Function_Group\"))\n",
    ")\n",
    "\n",
    "# Order by date\n",
    "opa_long_ordered = opa_long.orderBy(\"Der_Activity_Month_Date\")\n",
    "\n",
    "display(opa_long_ordered.limit(10))\n",
    "print(f\"Number of rows in opa_long_ordered: {opa_long_ordered.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bbdb2af-8682-41af-a49e-78981b93d63f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#12 – Aggregation and final metric derivation (Org, ICB, Region)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import when, col, lit\n",
    "\n",
    "# Start from Org-level counts from Container 10\n",
    "df_org = opa_final_with_added_metrics.withColumnRenamed(\"Adj Org Code\", \"Adj_Org_Code\")\n",
    "\n",
    "# Base metric columns to sum (added F2F_DNA which was missing)\n",
    "count_cols = [\n",
    "    \"All_Total\",\"All_First\",\"All_FU\",\"All_Proc\",\"All_NoProc\",\n",
    "    \"All_FU_Proc\",\"All_FU_NoProc\",\n",
    "    \"F2F_Total\",\"F2F_First\",\"F2F_FU\",\"F2F_DNA\",\n",
    "    \"Remote_Total\",\"Remote_First\",\"Remote_FU\",\"Remote_DNA\",\n",
    "    \"All_DNA\",\"All_First_DNA\",\"All_FU_DNA\",\n",
    "    \"All_2WW\",\"All_First_2WW\",\"All_FU_2WW\",\n",
    "    \"All_2WW_DNA\",\"All_First_2WW_DNA\",\"All_FU_2WW_DNA\"\n",
    "]\n",
    "\n",
    "# Function to (re)calculate rates & derived metrics (now includes F2F_DNA_Over_F2F_Total)\n",
    "def add_rate_metrics(df):\n",
    "    return (\n",
    "        df\n",
    "        # DNA metrics\n",
    "        .withColumn(\"All_DNA_Over_All_Total\", F.when((F.col(\"All_Total\")+F.col(\"All_DNA\"))!=0,\n",
    "            (F.col(\"All_DNA\")/(F.col(\"All_Total\")+F.col(\"All_DNA\")))*100).otherwise(None))\n",
    "        .withColumn(\"All_DNA_Over_All_Total_IG\", F.when((F.col(\"All_Total\")+F.col(\"All_DNA\"))!=0,\n",
    "            (F.col(\"All_DNA\")/(F.col(\"All_Total\")+F.col(\"All_DNA\")))*100).otherwise(None))\n",
    "        .withColumn(\"All_First_DNA_Over_All_First\", F.when((F.col(\"All_First\")+F.col(\"All_First_DNA\"))!=0,\n",
    "            (F.col(\"All_First_DNA\")/(F.col(\"All_First\")+F.col(\"All_First_DNA\")))*100).otherwise(None))\n",
    "        .withColumn(\"All_First_DNA_Over_All_First_IG\", F.when((F.col(\"All_First\")+F.col(\"All_First_DNA\"))!=0,\n",
    "            (F.col(\"All_First_DNA\")/(F.col(\"All_First\")+F.col(\"All_First_DNA\")))*100).otherwise(None))\n",
    "        .withColumn(\"All_FU_DNA_Over_All_FU\", F.when((F.col(\"All_FU\")+F.col(\"All_FU_DNA\"))!=0,\n",
    "            (F.col(\"All_FU_DNA\")/(F.col(\"All_FU\")+F.col(\"All_FU_DNA\")))*100).otherwise(None))\n",
    "        .withColumn(\"All_FU_DNA_Over_All_FU_IG\", F.when((F.col(\"All_FU\")+F.col(\"All_FU_DNA\"))!=0,\n",
    "            (F.col(\"All_FU_DNA\")/(F.col(\"All_FU\")+F.col(\"All_FU_DNA\")))*100).otherwise(None))\n",
    "        # FU metrics\n",
    "        .withColumn(\"All_FU_NoProc_Over_All_FU\", F.when(F.col(\"All_FU\")!=0,\n",
    "            (F.col(\"All_FU_NoProc\")/F.col(\"All_FU\"))*100).otherwise(None))\n",
    "        .withColumn(\"All_FU_Proc_Over_All_FU\", F.when(F.col(\"All_FU\")!=0,\n",
    "            (F.col(\"All_FU_Proc\")/F.col(\"All_FU\"))*100).otherwise(None))\n",
    "        .withColumn(\"All_FU_To_All_First\", F.when(F.col(\"All_First\")!=0,\n",
    "            (F.col(\"All_FU\")/F.col(\"All_First\"))).otherwise(None))\n",
    "        # 2WW rates\n",
    "        .withColumn(\"All_2WW_DNA_Over_All_2WW\", F.when(F.col(\"All_2WW\")!=0,\n",
    "            (F.col(\"All_2WW_DNA\")/F.col(\"All_2WW\"))*100).otherwise(None))\n",
    "        .withColumn(\"All_First_2WW_DNA_Over_All_First_2WW\", F.when(F.col(\"All_First_2WW\")!=0,\n",
    "            (F.col(\"All_First_2WW_DNA\")/F.col(\"All_First_2WW\"))*100).otherwise(None))\n",
    "        .withColumn(\"All_FU_2WW_DNA_Over_All_FU_2WW\", F.when(F.col(\"All_FU_2WW\")!=0,\n",
    "            (F.col(\"All_FU_2WW_DNA\")/F.col(\"All_FU_2WW\"))*100).otherwise(None))\n",
    "        # Mix shares\n",
    "        .withColumn(\"All_FU_Over_All_Total\", F.when(F.col(\"All_Total\")!=0,\n",
    "            (F.col(\"All_FU\")/F.col(\"All_Total\"))*100).otherwise(None))\n",
    "        .withColumn(\"All_First_Over_All_Total\", F.when(F.col(\"All_Total\")!=0,\n",
    "            (F.col(\"All_First\")/F.col(\"All_Total\"))*100).otherwise(None))\n",
    "        .withColumn(\"All_NoProc_Over_All_Total\", F.when(F.col(\"All_Total\")!=0,\n",
    "            (F.col(\"All_NoProc\")/F.col(\"All_Total\"))*100).otherwise(None))\n",
    "        .withColumn(\"All_Proc_Over_All_Total\", F.when(F.col(\"All_Total\")!=0,\n",
    "            (F.col(\"All_Proc\")/F.col(\"All_Total\"))*100).otherwise(None))\n",
    "        .withColumn(\"Remote_Total_Over_All_Total\", F.when(F.col(\"All_Total\")!=0,\n",
    "            (F.col(\"Remote_Total\")/F.col(\"All_Total\"))*100).otherwise(None))\n",
    "        .withColumn(\"Remote_FU_Over_All_FU\", F.when(F.col(\"All_FU\")!=0,\n",
    "            (F.col(\"Remote_FU\")/F.col(\"All_FU\"))*100).otherwise(None))\n",
    "        .withColumn(\"Remote_First_Over_All_First\", F.when(F.col(\"All_First\")!=0,\n",
    "            (F.col(\"Remote_First\")/F.col(\"All_First\"))*100).otherwise(None))\n",
    "        .withColumn(\"Remote_DNA_Over_Remote_Total\", F.when((F.col(\"Remote_Total\")+F.col(\"Remote_DNA\"))!=0,\n",
    "            (F.col(\"Remote_DNA\")/(F.col(\"Remote_Total\")+F.col(\"Remote_DNA\")))*100).otherwise(None))\n",
    "        .withColumn(\"F2F_DNA_Over_F2F_Total\", F.when((F.col(\"F2F_Total\")+F.col(\"F2F_DNA\"))!=0,\n",
    "            (F.col(\"F2F_DNA\")/(F.col(\"F2F_Total\")+F.col(\"F2F_DNA\")))*100).otherwise(None))\n",
    "    )\n",
    "\n",
    "# Aggregate to ICB\n",
    "df_icb = (\n",
    "    df_org.groupBy(\"Der_Activity_Month_Date\", \"ICB\", \"Treatment_Function_Group\")\n",
    "    .agg(*[F.sum(F.col(c)).alias(c) for c in count_cols])\n",
    ")\n",
    "df_icb = add_rate_metrics(df_icb).withColumn(\"Level\", F.lit(\"ICB\"))\n",
    "\n",
    "# Aggregate to Region\n",
    "df_region = (\n",
    "    df_org.groupBy(\"Der_Activity_Month_Date\", \"Region_Code\", \"Treatment_Function_Group\")\n",
    "    .agg(*[F.sum(F.col(c)).alias(c) for c in count_cols])\n",
    ")\n",
    "df_region = add_rate_metrics(df_region).withColumn(\"Level\", F.lit(\"Region\"))\n",
    "\n",
    "# Label Org-level rows\n",
    "df_org = df_org.withColumn(\"Level\", F.lit(\"Org\"))\n",
    "\n",
    "# Combine all levels into one dataset\n",
    "final_output = (\n",
    "    df_org.unionByName(df_icb, allowMissingColumns=True)\n",
    "          .unionByName(df_region, allowMissingColumns=True)\n",
    ")\n",
    "\n",
    "# Adjust codes based on Level\n",
    "final_output = final_output.withColumn(\n",
    "    \"Adj_Org_Code_Final\",\n",
    "    when(col(\"Level\") == \"Org\", col(\"Adj_Org_Code\"))\n",
    "    .when(col(\"Level\") == \"ICB\", col(\"ICB\"))\n",
    "    .when(col(\"Level\") == \"Region\", col(\"Region_Code\"))\n",
    ")\n",
    "\n",
    "# ---- Re-create \"simple copy\" and \"combo\" columns at ICB/Region so they’re populated, not NULL ----\n",
    "# >>> FIX ADDED: include All_First1/2/3 so they populate for ICB/Region <<<\n",
    "copy_map = {\n",
    "    \"All_DNA1\":\"All_DNA\",\"All_DNA2\":\"All_DNA\",\n",
    "    \"All_FU1\":\"All_FU\",\"All_FU2\":\"All_FU\",\"All_FU3\":\"All_FU\",\"All_FU4\":\"All_FU\",\"All_FU5\":\"All_FU\",\n",
    "    \"All_Total1\":\"All_Total\",\"All_Total2\":\"All_Total\",\"All_Total3\":\"All_Total\",\n",
    "    \"All_Total4\":\"All_Total\",\"All_Total5\":\"All_Total\",\"All_Total6\":\"All_Total\",\n",
    "    \"Remote_Total1\":\"Remote_Total\",\"Remote_Total2\":\"Remote_Total\",\n",
    "    \"All_First1\":\"All_First\",\"All_First2\":\"All_First\",\"All_First3\":\"All_First\"  # <-- fix\n",
    "}\n",
    "for newc, base in copy_map.items():\n",
    "    if base in final_output.columns:\n",
    "        final_output = final_output.withColumn(newc, F.col(base))\n",
    "\n",
    "combo_pairs = {\n",
    "    \"All_First_plus_All_First_DNA\": (\"All_First\",\"All_First_DNA\"),\n",
    "    \"All_FU_plus_All_FU_DNA\": (\"All_FU\",\"All_FU_DNA\"),\n",
    "    \"All_Total_plus_All_DNA\": (\"All_Total\",\"All_DNA\"),\n",
    "    \"F2F_Total_plus_F2F_DNA\": (\"F2F_Total\",\"F2F_DNA\"),\n",
    "    \"Remote_Total_plus_Remote_DNA\": (\"Remote_Total\",\"Remote_DNA\")\n",
    "}\n",
    "for newc, (a,b) in combo_pairs.items():\n",
    "    if a in final_output.columns and b in final_output.columns:\n",
    "        final_output = final_output.withColumn(newc, F.col(a).cast(\"long\") + F.col(b).cast(\"long\"))\n",
    "\n",
    "# Save (same as your original)\n",
    "final_output.write.mode(\"overwrite\").parquet(\"/mnt/output/opa_final_all_levels\")\n",
    "\n",
    "# display(final_output.limit(10))\n",
    "# print(f\"Rows in final output: {final_output.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc8aa02e-dd35-4b70-a01f-17d8baa7486c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#13 – long/skinny OPRT format (Level preserved)\n",
    "from pyspark.sql.functions import col, lit, explode, array, struct\n",
    "\n",
    "# Use the final_output table from container 12 (has Level + Adj_Org_Code_Final)\n",
    "df_wide = final_output\n",
    "\n",
    "# --- Step 1: Drop unnecessary columns (optional; keep if you don't need the raw counts downstream) ---\n",
    "cols_to_drop = [\n",
    "    \"Adj_Org_Code\",\n",
    "    \"All_DNA\", \"All_First\", \"All_FU\",\n",
    "    \"All_Total\", \"F2F_Total\", \"Remote_Total\"\n",
    "]\n",
    "df_wide = df_wide.drop(*[c for c in cols_to_drop if c in df_wide.columns])\n",
    "\n",
    "# --- Step 2: Define identifier columns (KEEP Level) ---\n",
    "id_cols = [\n",
    "    \"Der_Activity_Month_Date\",\n",
    "    \"Region_Code\",\n",
    "    \"ICB\",\n",
    "    \"Adj_Org_Code_Final\",\n",
    "    \"Treatment_Function_Group\",\n",
    "    \"Level\",  # <-- critical fix\n",
    "]\n",
    "\n",
    "# --- Step 3: Identify metric columns (exclude identifiers) ---\n",
    "metric_cols = [c for c in df_wide.columns if c not in id_cols]\n",
    "\n",
    "# --- Step 4: Melt into long/skinny format ---\n",
    "opa_oprt_long = (\n",
    "    df_wide.select(\n",
    "        *id_cols,\n",
    "        explode(array(*[\n",
    "            struct(lit(c).alias(\"OPRT_Metric_Name\"), col(c).alias(\"Metric_Value\"))\n",
    "            for c in metric_cols\n",
    "        ])).alias(\"kv\")\n",
    "    )\n",
    "    .select(\n",
    "        *id_cols,\n",
    "        col(\"kv.OPRT_Metric_Name\"),\n",
    "        col(\"kv.Metric_Value\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- Step 5: Optional renaming (kept for consistency) ---\n",
    "opa_oprt_long = opa_oprt_long.withColumnRenamed(\"Treatment_Function_Group\", \"Treatment_Function_Group\")\n",
    "\n",
    "display(opa_oprt_long.limit(10))\n",
    "print(f\"Container 13 complete — {opa_oprt_long.count()} rows, {len(opa_oprt_long.columns)} columns\")\n",
    "\n",
    "# Ensure Container 14 uses the cleaned long-format table\n",
    "final_output = opa_oprt_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a17e7bc2-c4e7-41b4-a017-b261ac96fc62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#14 — Join Internal IDs and Clean Long OPRT Dataset (no restacking; Level preserved)\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, concat_ws, lower, regexp_replace, trim\n",
    ")\n",
    "from pyspark.sql.types import StringType, DoubleType, DateType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# --- Step 1: Start from container 13 output (already has Level + Adj_Org_Code_Final) ---\n",
    "df_long = final_output\n",
    "\n",
    "# --- Step 2: DO NOT restack; just keep the dataset as-is ---\n",
    "df_stacked = df_long\n",
    "\n",
    "# --- Step 3: Filter out unwanted Treatment_Function_Group = 'Other' ---\n",
    "df_stacked = df_stacked.filter(col(\"Treatment_Function_Group\") != \"Other\")\n",
    "\n",
    "# --- Step 4: Build combined metric name for joining to ID list ---\n",
    "df_stacked = df_stacked.withColumn(\n",
    "    \"OPRT_Metric_Name_TFC\",\n",
    "    concat_ws(\"_\", col(\"OPRT_Metric_Name\"), col(\"Treatment_Function_Group\"))\n",
    ")\n",
    "\n",
    "# --- Step 5: Normalize join keys on our long dataset ---\n",
    "# spaces -> underscores, drop non [a-z0-9_], collapse underscores, trim underscores, lowercase\n",
    "df_stacked_clean = df_stacked.withColumn(\n",
    "    \"join_metric\",\n",
    "    lower(regexp_replace(trim(col(\"OPRT_Metric_Name_TFC\")), r\"\\s+\", \"_\"))\n",
    ")\n",
    "df_stacked_clean = df_stacked_clean.withColumn(\n",
    "    \"join_metric\",\n",
    "    regexp_replace(col(\"join_metric\"), r\"[^a-z0-9_]\", \"\")\n",
    ")\n",
    "df_stacked_clean = df_stacked_clean.withColumn(\n",
    "    \"join_metric\",\n",
    "    regexp_replace(col(\"join_metric\"), r\"_+\", \"_\")\n",
    ")\n",
    "df_stacked_clean = df_stacked_clean.withColumn(\n",
    "    \"join_metric\",\n",
    "    regexp_replace(col(\"join_metric\"), r\"^_+|_+$\", \"\")\n",
    ")\n",
    "\n",
    "# --- Step 5b: Prepare mhs_metric_list cleaned keys (bring in Description too) ---\n",
    "metric_list_col = [c for c in mhs_metric_list.columns if \"OPRT\" in c and \"TFC\" in c]\n",
    "if len(metric_list_col) == 0:\n",
    "    raise ValueError(\"Could not find OPRT TFC metric column name in mhs_metric_list. Review mhs_metric_list columns.\")\n",
    "metric_list_col = metric_list_col[0]\n",
    "\n",
    "mhs_metric_list_clean = mhs_metric_list.withColumn(\n",
    "    \"join_metric\",\n",
    "    lower(regexp_replace(trim(col(metric_list_col)), r\"\\s+\", \"_\"))\n",
    ")\n",
    "mhs_metric_list_clean = mhs_metric_list_clean.withColumn(\n",
    "    \"join_metric\",\n",
    "    regexp_replace(col(\"join_metric\"), r\"[^a-z0-9_]\", \"\")\n",
    ")\n",
    "mhs_metric_list_clean = mhs_metric_list_clean.withColumn(\n",
    "    \"join_metric\",\n",
    "    regexp_replace(col(\"join_metric\"), r\"_+\", \"_\")\n",
    ")\n",
    "mhs_metric_list_clean = mhs_metric_list_clean.withColumn(\n",
    "    \"join_metric\",\n",
    "    regexp_replace(col(\"join_metric\"), r\"^_+|_+$\", \"\")\n",
    ")\n",
    "\n",
    "select_cols = [\"join_metric\", \"InternalID\"]\n",
    "if \"Description\" in mhs_metric_list_clean.columns:\n",
    "    select_cols.append(\"Description\")\n",
    "else:\n",
    "    print(\"WARNING: mhs_metric_list does not contain a column named 'Description'.\")\n",
    "\n",
    "mhs_metric_list_clean_sel = mhs_metric_list_clean.select(*select_cols).distinct()\n",
    "\n",
    "# --- Step 6a: Left join first — to capture unmatched metrics for debugging ---\n",
    "df_with_id_left = df_stacked_clean.join(\n",
    "    mhs_metric_list_clean_sel,\n",
    "    on=\"join_metric\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Diagnostics (optional)\n",
    "unmatched = df_with_id_left.filter(col(\"InternalID\").isNull()).select(\"OPRT_Metric_Name_TFC\").distinct()\n",
    "# unmatched.write.mode(\"overwrite\").parquet(\"/mnt/output/unmatched_oprt_metrics.parquet\")\n",
    "# df_with_id_left.filter(lower(col(\"OPRT_Metric_Name\")).like(\"%2ww%\")) \\\n",
    "#     .select(\"OPRT_Metric_Name_TFC\",\"InternalID\").distinct().show(200, truncate=False)\n",
    "\n",
    "# --- Step 6b: Final join for production (inner; drop unmapped) ---\n",
    "df_with_id = df_stacked_clean.join(\n",
    "    mhs_metric_list_clean_sel,\n",
    "    on=\"join_metric\",\n",
    "    how=\"inner\"\n",
    ").drop(\"join_metric\")\n",
    "\n",
    "# --- Step 7: Filter allowable org codes if provided ---\n",
    "if \"Org_Code\" in mhs_allowable_orgs.columns:\n",
    "    df_with_id = df_with_id.join(\n",
    "        mhs_allowable_orgs.select(col(\"Org_Code\").alias(\"allowable_code\")),\n",
    "        df_with_id[\"Adj_Org_Code_Final\"] == col(\"allowable_code\"),\n",
    "        \"inner\"\n",
    "    ).drop(\"allowable_code\")\n",
    "\n",
    "# --- Step 8: Enforce clean data types and ensure Description present ---\n",
    "df_with_id = (\n",
    "    df_with_id\n",
    "    .withColumn(\"Der_Activity_Month_Date\", col(\"Der_Activity_Month_Date\").cast(DateType()))\n",
    "    .withColumn(\"Adj_Org_Code_Final\", col(\"Adj_Org_Code_Final\").cast(StringType()))\n",
    "    .withColumn(\"Level\", col(\"Level\").cast(StringType()))\n",
    "    .withColumn(\"Treatment_Function_Group\", col(\"Treatment_Function_Group\").cast(StringType()))\n",
    "    .withColumn(\"OPRT_Metric_Name\", col(\"OPRT_Metric_Name\").cast(StringType()))\n",
    "    .withColumn(\"OPRT_Metric_Name_TFC\", col(\"OPRT_Metric_Name_TFC\").cast(StringType()))\n",
    "    .withColumn(\"Metric_Value\", col(\"Metric_Value\").cast(DoubleType()))\n",
    ")\n",
    "\n",
    "if \"Description\" not in df_with_id.columns:\n",
    "    df_with_id = df_with_id.withColumn(\"Description\", lit(None).cast(StringType()))\n",
    "\n",
    "# --- Step 9: Write final tidy dataset ---\n",
    "df_with_id.write.mode(\"overwrite\").parquet(\"/mnt/output/opa_oprt_final\")\n",
    "\n",
    "# display(df_with_id.limit(20))\n",
    "# print(f\" Container 14 complete — {df_with_id.count()} rows, {len(df_with_id.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acc65f15-e5a2-45db-9a01-2685810ed977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#15 – Add Remote Lower Benchmark\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Start from container 10 output\n",
    "df_benchmark = opa_final_with_added_metrics\n",
    "\n",
    "# Step 1: Calculate 25th percentile of Remote_Total by month and Adj Org Code\n",
    "remote_lower = (\n",
    "    df_benchmark\n",
    "    .groupBy(\"Der_Activity_Month_Date\", \"Adj Org Code\")\n",
    "    .agg(\n",
    "        F.expr(\"percentile_approx(Remote_Total, 0.25)\").alias(\"Remote_Lower_Benchmark\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 2: Join benchmark back to main dataset\n",
    "df_with_benchmark = df_benchmark.join(\n",
    "    remote_lower,\n",
    "    on=[\"Der_Activity_Month_Date\", \"Adj Org Code\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Step 3: For missing values, fill with 0\n",
    "df_with_benchmark = df_with_benchmark.fillna({\"Remote_Lower_Benchmark\": 0})\n",
    "\n",
    "# Step 4: Final outputs\n",
    "opa_final_with_remote_benchmark = df_with_benchmark\n",
    "\n",
    "# Create a standalone lower benchmark table for downstream use\n",
    "df_lower_bm = (\n",
    "    remote_lower\n",
    "    .withColumnRenamed(\"Adj Org Code\", \"Adj_Org_Code_Final\")\n",
    "    .select(\"Der_Activity_Month_Date\", \"Adj_Org_Code_Final\", \"Remote_Lower_Benchmark\")\n",
    ")\n",
    "\n",
    "# Optional: Save for reference or later join\n",
    "df_lower_bm.write.mode(\"overwrite\").parquet(\"/mnt/output/opa_lower_benchmark\")\n",
    "\n",
    "#display(opa_final_with_remote_benchmark.limit(10))\n",
    "#print(f\"Container 15 complete — {opa_final_with_remote_benchmark.count()} rows, {len(opa_final_with_remote_benchmark.columns)} columns\")\n",
    "#print(f\"Lower benchmark table created — {df_lower_bm.count()} rows, {len(df_lower_bm.columns)} columns\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c00711f-0354-4c39-9bf7-7688e1ca355c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#16 DNA opportunities\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Start from container 10 output (with All_DNA_Over_All_Total metric)\n",
    "df = opa_final_with_added_metrics\n",
    "\n",
    "# Add Level column if missing (Org level)\n",
    "if \"Level\" not in df.columns:\n",
    "    df = df.withColumn(\"Level\", F.lit(\"Org\"))\n",
    "\n",
    "# Ignore Treatment_Function_Group and focus on Org/Level\n",
    "df_filtered = df.select(\n",
    "    \"Der_Activity_Month_Date\",\n",
    "    \"Adj Org Code\",\n",
    "    \"Level\",\n",
    "    \"All_DNA\",\n",
    "    \"All_Total\",\n",
    "    \"All_DNA_Over_All_Total\"\n",
    ")\n",
    "\n",
    "# Define a 6-month rolling window per Org/Level\n",
    "window_spec = (\n",
    "    Window.partitionBy(\"Adj Org Code\", \"Level\")\n",
    "          .orderBy(F.col(\"Der_Activity_Month_Date\").cast(\"long\"))\n",
    "          .rowsBetween(-5, 0)  # last 6 months including current\n",
    ")\n",
    "\n",
    "# Calculate rolling averages\n",
    "avg_df = df_filtered.withColumn(\n",
    "    \"Avg_All_DNA\", F.avg(\"All_DNA\").over(window_spec)\n",
    ").withColumn(\n",
    "    \"Avg_DNA_rate\", F.avg(\"All_DNA_Over_All_Total\").over(window_spec)\n",
    ")\n",
    "\n",
    "# Calculate national median and 25th percentile for each month\n",
    "national_stats = (\n",
    "    df_filtered.groupBy(\"Der_Activity_Month_Date\")\n",
    "    .agg(\n",
    "        F.expr(\"percentile_approx(All_DNA_Over_All_Total, 0.5)\").alias(\"National_Median\"),\n",
    "        F.expr(\"percentile_approx(All_DNA_Over_All_Total, 0.25)\").alias(\"Percentile_25\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Join stats back to avg_df\n",
    "avg_df = avg_df.join(\n",
    "    national_stats,\n",
    "    on=\"Der_Activity_Month_Date\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Apply DNA_Opportunity_reduction rules\n",
    "avg_df = avg_df.withColumn(\n",
    "    \"DNA_Opportunity_reduction\",\n",
    "    F.when(F.col(\"Avg_DNA_rate\").isNull(), \"No reduction\")\n",
    "     .when(F.col(\"Avg_All_DNA\").isNull() | (F.col(\"Avg_All_DNA\") == 0), \"No reduction\")\n",
    "     .when(F.col(\"Avg_DNA_rate\") > F.col(\"National_Median\"), \"25% reduction\")\n",
    "     .when((F.col(\"Avg_DNA_rate\") <= F.col(\"National_Median\")) & (F.col(\"Avg_DNA_rate\") > F.col(\"Percentile_25\")), \"15% reduction\")\n",
    "     .when(F.col(\"Avg_DNA_rate\") <= F.col(\"Percentile_25\"), \"10% reduction\")\n",
    "     .otherwise(None)\n",
    ")\n",
    "\n",
    "# Calculate DNA_Opportunity as an integer\n",
    "avg_df = avg_df.withColumn(\n",
    "    \"DNA_Opportunity\",\n",
    "    F.when(F.col(\"DNA_Opportunity_reduction\") == \"No reduction\", F.lit(0))\n",
    "     .when(F.col(\"DNA_Opportunity_reduction\") == \"25% reduction\", F.round(0.25 * F.col(\"Avg_All_DNA\")).cast(\"int\"))\n",
    "     .when(F.col(\"DNA_Opportunity_reduction\") == \"15% reduction\", F.round(0.15 * F.col(\"Avg_All_DNA\")).cast(\"int\"))\n",
    "     .when(F.col(\"DNA_Opportunity_reduction\") == \"10% reduction\", F.round(0.10 * F.col(\"Avg_All_DNA\")).cast(\"int\"))\n",
    "     .otherwise(F.lit(None))\n",
    ")\n",
    "\n",
    "# Optional: keep only one row per Org/Level for latest month\n",
    "latest_month_window = Window.partitionBy(\"Adj Org Code\", \"Level\").orderBy(F.col(\"Der_Activity_Month_Date\").desc())\n",
    "dna_opp_df = avg_df.withColumn(\"row_num\", F.row_number().over(latest_month_window)).filter(F.col(\"row_num\") == 1).drop(\"row_num\")\n",
    "\n",
    "display(dna_opp_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ea1c16f-da27-4a67-9b32-8d23e056d65c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#17 — Validation & Reconciliation Suite (final, syntax-safe)\n",
    "# Uses:\n",
    "# - /mnt/output/opa_final_all_levels  (wide Org/ICB/Region from Container 12)\n",
    "# - opa_final_with_added_metrics      (Container 10 output, org-level wide)\n",
    "# - df_master_hierarchies, df_icb_region (for fan-out checks)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, sum as s, lit\n",
    "\n",
    "# -------- Load wide combined output (since Container 13 overwrote final_output to long) --------\n",
    "final_output_wide = spark.read.parquet(\"/mnt/output/opa_final_all_levels\")\n",
    "\n",
    "# -------- Helpers --------\n",
    "def show_sample(df, n=10, title=None):\n",
    "    if title:\n",
    "        print(\"\\n\" + \"=\" * len(title))\n",
    "        print(title)\n",
    "        print(\"=\" * len(title))\n",
    "    display(df.limit(n))\n",
    "\n",
    "def count_failures(df, label):\n",
    "    c = df.count()\n",
    "    print(f\"{label}: {c} failing row(s)\")\n",
    "    return c\n",
    "\n",
    "# Pick recent months defensively\n",
    "month_sel = (\n",
    "    final_output_wide.select(\"Der_Activity_Month_Date\")\n",
    "    .where(col(\"Der_Activity_Month_Date\").isNotNull())\n",
    "    .distinct()\n",
    "    .orderBy(F.desc(\"Der_Activity_Month_Date\"))\n",
    "    .limit(3)\n",
    ")\n",
    "sample_months = [r[0] for r in month_sel.collect()]\n",
    "\n",
    "# -------- 1) Identity checks on org-level wide (Container 10) --------\n",
    "org_base = opa_final_with_added_metrics\n",
    "\n",
    "checks = []\n",
    "\n",
    "checks.append((\n",
    "    \"All_Total == All_First + All_FU\",\n",
    "    org_base.filter(\n",
    "        (col(\"All_Total\").cast(\"long\") != (col(\"All_First\").cast(\"long\") + col(\"All_FU\").cast(\"long\")))\n",
    "        | col(\"All_Total\").isNull() | col(\"All_First\").isNull() | col(\"All_FU\").isNull()\n",
    "    )\n",
    "))\n",
    "\n",
    "checks.append((\n",
    "    \"All_Proc + All_NoProc == All_Total\",\n",
    "    org_base.filter(\n",
    "        ((col(\"All_Proc\").cast(\"long\") + col(\"All_NoProc\").cast(\"long\")) != col(\"All_Total\").cast(\"long\"))\n",
    "        | col(\"All_Proc\").isNull() | col(\"All_NoProc\").isNull() | col(\"All_Total\").isNull()\n",
    "    )\n",
    "))\n",
    "\n",
    "checks.append((\n",
    "    \"F2F_Total == F2F_First + F2F_FU\",\n",
    "    org_base.filter(\n",
    "        (col(\"F2F_Total\").cast(\"long\") != (col(\"F2F_First\").cast(\"long\") + col(\"F2F_FU\").cast(\"long\")))\n",
    "        | col(\"F2F_Total\").isNull() | col(\"F2F_First\").isNull() | col(\"F2F_FU\").isNull()\n",
    "    )\n",
    "))\n",
    "\n",
    "checks.append((\n",
    "    \"Remote_Total == Remote_First + Remote_FU\",\n",
    "    org_base.filter(\n",
    "        (col(\"Remote_Total\").cast(\"long\") != (col(\"Remote_First\").cast(\"long\") + col(\"Remote_FU\").cast(\"long\")))\n",
    "        | col(\"Remote_Total\").isNull() | col(\"Remote_First\").isNull() | col(\"Remote_FU\").isNull()\n",
    "    )\n",
    "))\n",
    "\n",
    "checks.append((\n",
    "    \"All_2WW == All_First_2WW + All_FU_2WW\",\n",
    "    org_base.filter(\n",
    "        (col(\"All_2WW\").cast(\"long\") != (col(\"All_First_2WW\").cast(\"long\") + col(\"All_FU_2WW\").cast(\"long\")))\n",
    "        | col(\"All_2WW\").isNull() | col(\"All_First_2WW\").isNull() | col(\"All_FU_2WW\").isNull()\n",
    "    )\n",
    "))\n",
    "\n",
    "checks.append((\n",
    "    \"All_2WW_DNA == All_First_2WW_DNA + All_FU_2WW_DNA\",\n",
    "    org_base.filter(\n",
    "        (col(\"All_2WW_DNA\").cast(\"long\") != (col(\"All_First_2WW_DNA\").cast(\"long\") + col(\"All_FU_2WW_DNA\").cast(\"long\")))\n",
    "        | col(\"All_2WW_DNA\").isNull() | col(\"All_First_2WW_DNA\").isNull() | col(\"All_FU_2WW_DNA\").isNull()\n",
    "    )\n",
    "))\n",
    "\n",
    "print(\"\\n=== Identity checks on Org-level (Container 10) ===\")\n",
    "for label, failing in checks:\n",
    "    cnt = count_failures(failing, label)\n",
    "    if cnt > 0:\n",
    "        show_sample(\n",
    "            failing.select(\n",
    "                \"Der_Activity_Month_Date\", \"Adj Org Code\", \"Treatment_Function_Group\",\n",
    "                \"All_Total\", \"All_First\", \"All_FU\", \"All_Proc\", \"All_NoProc\",\n",
    "                \"F2F_Total\", \"F2F_First\", \"F2F_FU\",\n",
    "                \"Remote_Total\", \"Remote_First\", \"Remote_FU\",\n",
    "                \"All_2WW\", \"All_First_2WW\", \"All_FU_2WW\",\n",
    "                \"All_2WW_DNA\", \"All_First_2WW_DNA\", \"All_FU_2WW_DNA\"\n",
    "            ),\n",
    "            n=15,\n",
    "            title=label + \" — examples\"\n",
    "        )\n",
    "\n",
    "# -------- 2) Join fan-out checks --------\n",
    "hier_dupes = (\n",
    "    df_master_hierarchies\n",
    "    .groupBy(\"Organisation_Code\")\n",
    "    .agg(F.countDistinct(\"STP_Code\").alias(\"icb_count\"))\n",
    "    .filter(col(\"icb_count\") > 1)\n",
    ")\n",
    "print(\"\\n=== Hierarchy fan-out check (Org -> ICB) ===\")\n",
    "count_failures(hier_dupes, \"Orgs mapping to >1 ICB\")\n",
    "\n",
    "icb_dupes = (\n",
    "    df_icb_region\n",
    "    .groupBy(\"ICB_Code\")\n",
    "    .agg(F.countDistinct(\"Region_Code\").alias(\"region_count\"))\n",
    "    .filter(col(\"region_count\") > 1)\n",
    ")\n",
    "print(\"\\n=== ICB->Region fan-out check ===\")\n",
    "count_failures(icb_dupes, \"ICBs mapping to >1 Region\")\n",
    "\n",
    "# -------- 3) Rollup reconciliation (Org -> ICB -> Region) using wide data --------\n",
    "print(\"\\n=== Rollup reconciliation (Org -> ICB -> Region) using wide data ===\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7a9351e-16e2-458a-a7bd-669fde6d9b50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#18  — Explicit rollup delta report (all months) + hard assert on latest\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "wide = spark.read.parquet(\"/mnt/output/opa_final_all_levels\")\n",
    "org = opa_final_with_added_metrics\n",
    "\n",
    "# make sure we actually have dates\n",
    "months = (wide.select(\"Der_Activity_Month_Date\")\n",
    "          .where(col(\"Der_Activity_Month_Date\").isNotNull())\n",
    "          .distinct()\n",
    "          .orderBy(\"Der_Activity_Month_Date\"))\n",
    "print(f\"Found {months.count()} month(s) in wide output.\")\n",
    "\n",
    "keys = [\"All_Total\",\"All_First\",\"All_FU\",\"F2F_Total\",\"Remote_Total\",\"All_DNA\",\"All_First_DNA\",\"All_FU_DNA\"]\n",
    "\n",
    "# --- Org -> ICB deltas per month ---\n",
    "org_icb = (\n",
    "    org.groupBy(\"Der_Activity_Month_Date\",\"ICB\",\"Treatment_Function_Group\")\n",
    "       .agg(*[F.sum(col(c)).alias(c) for c in keys])\n",
    ")\n",
    "\n",
    "icb_rows = (\n",
    "    wide.filter(col(\"Level\")==\"ICB\")\n",
    "        .select(\"Der_Activity_Month_Date\",\"ICB\",\"Treatment_Function_Group\",*keys)\n",
    ")\n",
    "\n",
    "org_icb_delta = (\n",
    "    org_icb.alias(\"a\").join(icb_rows.alias(\"b\"),\n",
    "        on=[\"Der_Activity_Month_Date\",\"ICB\",\"Treatment_Function_Group\"], how=\"full\")\n",
    "    .select(\n",
    "        \"Der_Activity_Month_Date\",\n",
    "        *[(col(f\"a.{c}\")-col(f\"b.{c}\")).alias(f\"delta_{c}\") for c in keys]\n",
    "    )\n",
    ")\n",
    "\n",
    "org_icb_summary = org_icb_delta.groupBy(\"Der_Activity_Month_Date\").agg(\n",
    "    *[F.max(F.abs(col(f\"delta_{c}\"))).alias(f\"ORG2ICB_maxabs_{c}\") for c in keys]\n",
    ")\n",
    "\n",
    "# --- ICB -> Region deltas per month ---\n",
    "icb_sum = (\n",
    "    wide.filter(col(\"Level\")==\"ICB\")\n",
    "        .groupBy(\"Der_Activity_Month_Date\",\"Region_Code\",\"Treatment_Function_Group\")\n",
    "        .agg(*[F.sum(col(c)).alias(c) for c in keys])\n",
    ")\n",
    "\n",
    "region_rows = (\n",
    "    wide.filter(col(\"Level\")==\"Region\")\n",
    "        .select(\"Der_Activity_Month_Date\",\"Region_Code\",\"Treatment_Function_Group\",*keys)\n",
    ")\n",
    "\n",
    "icb_region_delta = (\n",
    "    icb_sum.alias(\"a\").join(region_rows.alias(\"b\"),\n",
    "        on=[\"Der_Activity_Month_Date\",\"Region_Code\",\"Treatment_Function_Group\"], how=\"full\")\n",
    "    .select(\n",
    "        \"Der_Activity_Month_Date\",\n",
    "        *[(col(f\"a.{c}\")-col(f\"b.{c}\")).alias(f\"delta_{c}\") for c in keys]\n",
    "    )\n",
    ")\n",
    "\n",
    "icb_region_summary = icb_region_delta.groupBy(\"Der_Activity_Month_Date\").agg(\n",
    "    *[F.max(F.abs(col(f\"delta_{c}\"))).alias(f\"ICB2REG_maxabs_{c}\") for c in keys]\n",
    ")\n",
    "\n",
    "# --- Show compact table of max |delta| per month ---\n",
    "summary = (\n",
    "    org_icb_summary.alias(\"x\")\n",
    "    .join(icb_region_summary.alias(\"y\"), on=\"Der_Activity_Month_Date\", how=\"outer\")\n",
    "    .orderBy(\"Der_Activity_Month_Date\")\n",
    ")\n",
    "\n",
    "print(\"\\n=== Max |delta| by month (should all be 0 if perfectly consistent) ===\")\n",
    "display(summary)\n",
    "\n",
    "# --- Hard assert on latest month ---\n",
    "latest = summary.orderBy(F.desc(\"Der_Activity_Month_Date\")).first()\n",
    "if latest:\n",
    "    row = latest.asDict()\n",
    "    bad = {k:v for k,v in row.items() if k!=\"Der_Activity_Month_Date\" and v not in (0,0.0,None)}\n",
    "    if bad:\n",
    "        raise AssertionError(f\"Non-zero rollup deltas for latest month {row['Der_Activity_Month_Date']}: {bad}\")\n",
    "    else:\n",
    "        print(f\"✅ Hard assertion passed for latest month {row['Der_Activity_Month_Date']}: all max |delta| == 0\")\n",
    "else:\n",
    "    print(\"No months found to assert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "792acef0-e51c-4c4a-9bd7-d1af883640c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#19 — Post-run QA (robust): ICB/Region completeness for copy/combos & F2F DNA metrics\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"=== Container 17: Post-run QA (read-only, robust) ===\")\n",
    "\n",
    "# ---------- 0) Try to read wide output from C12 (authoritative for column-based checks) ----------\n",
    "df_wide = None\n",
    "df_long = None\n",
    "\n",
    "# Load wide (C12)\n",
    "try:\n",
    "    df_wide = spark.read.parquet(\"/mnt/output/opa_final_all_levels\")\n",
    "    print(\"Loaded wide table from /mnt/output/opa_final_all_levels\")\n",
    "except Exception as e:\n",
    "    print(\"Could not load wide table (/mnt/output/opa_final_all_levels):\", str(e))\n",
    "\n",
    "# Load long (C14) for secondary checks\n",
    "try:\n",
    "    df_long = spark.read.parquet(\"/mnt/output/opa_oprt_final\")\n",
    "    print(\"Loaded long table from /mnt/output/opa_oprt_final\")\n",
    "except Exception as e:\n",
    "    if 'df_with_id' in locals():\n",
    "        df_long = df_with_id\n",
    "        print(\"Using in-memory df_with_id for long checks\")\n",
    "    else:\n",
    "        print(\"No long table available for OPRT-based checks.\")\n",
    "\n",
    "# ---------- 1) If we have the wide table, run column-based checks there ----------\n",
    "copy_cols  = [\n",
    "    \"All_DNA1\",\"All_DNA2\",\n",
    "    \"All_FU1\",\"All_FU2\",\"All_FU3\",\"All_FU4\",\"All_FU5\",\n",
    "    \"All_Total1\",\"All_Total2\",\"All_Total3\",\"All_Total4\",\"All_Total5\",\"All_Total6\",\n",
    "    \"Remote_Total1\",\"Remote_Total2\"\n",
    "]\n",
    "combo_cols = [\n",
    "    \"All_First_plus_All_First_DNA\",\n",
    "    \"All_FU_plus_All_FU_DNA\",\n",
    "    \"All_Total_plus_All_DNA\",\n",
    "    \"F2F_Total_plus_F2F_DNA\",\n",
    "    \"Remote_Total_plus_Remote_DNA\"\n",
    "]\n",
    "extra_cols = [\"F2F_DNA\", \"F2F_DNA_Over_F2F_Total\"]\n",
    "\n",
    "def null_summary(df, label, cols):\n",
    "    total = df.count()\n",
    "    print(f\"\\n--- {label}: {total:,} rows ---\")\n",
    "    if total == 0:\n",
    "        print(\"No rows to check.\")\n",
    "        return\n",
    "    cols_present = [c for c in cols if c in df.columns]\n",
    "    cols_missing = sorted(set(cols) - set(cols_present))\n",
    "    if cols_missing:\n",
    "        print(\"WARNING: Missing expected columns:\", cols_missing)\n",
    "    if not cols_present:\n",
    "        print(\"No expected columns present — skipping null summary.\")\n",
    "        return\n",
    "    exprs = [F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c) for c in cols_present]\n",
    "    res = df.agg(*exprs).collect()[0].asDict()\n",
    "    any_nulls = [(k, int(v), round(100*float(v)/total,2)) for k,v in res.items() if v and v > 0]\n",
    "    if not any_nulls:\n",
    "        print(\"All checked columns are fully populated (no NULLs). ✅\")\n",
    "    else:\n",
    "        print(\"Columns with NULLs (count, % of rows):\")\n",
    "        for k, v, pct in sorted(any_nulls, key=lambda x: (-x[1], x[0])):\n",
    "            print(f\"  {k:30s} {v:8d} ({pct:5.2f}%)\")\n",
    "\n",
    "def spotcheck_copies(df, label, n=100):\n",
    "    print(f\"\\n--- Spot-check copy columns @ {label} (random {n} rows) ---\")\n",
    "    mapping = {\n",
    "        \"All_DNA1\":\"All_DNA\",\"All_DNA2\":\"All_DNA\",\n",
    "        \"All_FU1\":\"All_FU\",\"All_FU2\":\"All_FU\",\"All_FU3\":\"All_FU\",\"All_FU4\":\"All_FU\",\"All_FU5\":\"All_FU\",\n",
    "        \"All_Total1\":\"All_Total\",\"All_Total2\":\"All_Total\",\"All_Total3\":\"All_Total\",\n",
    "        \"All_Total4\":\"All_Total\",\"All_Total5\":\"All_Total\",\"All_Total6\":\"All_Total\",\n",
    "        \"Remote_Total1\":\"Remote_Total\",\"Remote_Total2\":\"Remote_Total\"\n",
    "    }\n",
    "    pairs = [(k,v) for k,v in mapping.items() if k in df.columns and v in df.columns]\n",
    "    if not pairs:\n",
    "        print(\"No copy columns present — skipping.\")\n",
    "        return\n",
    "    sample = df.orderBy(F.rand()).limit(n)\n",
    "    exprs = [F.sum(F.when(F.col(k) != F.col(v), 1).otherwise(0)).alias(k+\"_neq\") for k,v in pairs]\n",
    "    out = sample.agg(*exprs).collect()[0].asDict()\n",
    "    bad = [(k,v) for k,v in out.items() if v and v > 0]\n",
    "    if not bad:\n",
    "        print(\"All copy columns match their base values in the sample. ✅\")\n",
    "    else:\n",
    "        print(\"Mismatches found in sample:\")\n",
    "        for k,v in bad:\n",
    "            print(f\"  {k}: {v}\")\n",
    "\n",
    "def spotcheck_combos(df, label, n=100):\n",
    "    print(f\"\\n--- Spot-check combo columns @ {label} (random {n} rows) ---\")\n",
    "    pairs = {\n",
    "        \"All_First_plus_All_First_DNA\": (\"All_First\",\"All_First_DNA\"),\n",
    "        \"All_FU_plus_All_FU_DNA\": (\"All_FU\",\"All_FU_DNA\"),\n",
    "        \"All_Total_plus_All_DNA\": (\"All_Total\",\"All_DNA\"),\n",
    "        \"F2F_Total_plus_F2F_DNA\": (\"F2F_Total\",\"F2F_DNA\"),\n",
    "        \"Remote_Total_plus_Remote_DNA\": (\"Remote_Total\",\"Remote_DNA\")\n",
    "    }\n",
    "    usable = [(newc,a,b) for newc,(a,b) in pairs.items() if all(c in df.columns for c in [newc,a,b])]\n",
    "    if not usable:\n",
    "        print(\"No combo columns present — skipping.\")\n",
    "        return\n",
    "    sample = df.orderBy(F.rand()).limit(n)\n",
    "    exprs = [F.sum(F.when(F.col(newc) != (F.col(a).cast(\"long\")+F.col(b).cast(\"long\")), 1).otherwise(0)).alias(newc+\"_neq\")\n",
    "             for (newc,a,b) in usable]\n",
    "    out = sample.agg(*exprs).collect()[0].asDict()\n",
    "    bad = [(k,v) for k,v in out.items() if v and v > 0]\n",
    "    if not bad:\n",
    "        print(\"All combo columns equal the sum of their parts in the sample. ✅\")\n",
    "    else:\n",
    "        print(\"Mismatches found in sample:\")\n",
    "        for k,v in bad:\n",
    "            print(f\"  {k}: {v}\")\n",
    "\n",
    "# Replace the old rate_cols(...) in Container 17 with this:\n",
    "def rate_cols(df):\n",
    "    # pick only columns that clearly represent rates (contain '_Over_') or are explicitly named rate fields\n",
    "    explicit = {\"F2F_DNA_Over_F2F_Total\", \"Remote_DNA_Over_Remote_Total\"}\n",
    "    return sorted([c for c in df.columns if (\"_Over_\" in c) or (c in explicit)])\n",
    "\n",
    "\n",
    "def rate_bounds(df, label):\n",
    "    rcols = rate_cols(df)\n",
    "    if not rcols:\n",
    "        print(f\"\\n--- {label}: No rate columns detected — skipping. ---\")\n",
    "        return\n",
    "    print(f\"\\n--- {label}: rate bounds (0–100) ---\")\n",
    "    exprs = [F.sum(F.when((F.col(c) < 0) | (F.col(c) > 100), 1).otherwise(0)).alias(c) for c in rcols]\n",
    "    out = df.agg(*exprs).collect()[0].asDict()\n",
    "    bad = [(k,v) for k,v in out.items() if v and v > 0]\n",
    "    if not bad:\n",
    "        print(\"All rates within [0,100] (ignoring NULLs). ✅\")\n",
    "    else:\n",
    "        print(\"Out-of-range rate values detected:\")\n",
    "        for k,v in bad:\n",
    "            print(f\"  {k}: {v}\")\n",
    "\n",
    "if df_wide is not None:\n",
    "    df_icb    = df_wide.filter(F.col(\"Level\") == \"ICB\")\n",
    "    df_region = df_wide.filter(F.col(\"Level\") == \"Region\")\n",
    "\n",
    "    # 1A) Null coverage\n",
    "    cols_to_check = [c for c in (copy_cols + combo_cols + extra_cols) if c in df_wide.columns]\n",
    "    null_summary(df_icb, \"ICB level (wide)\", cols_to_check)\n",
    "    null_summary(df_region, \"Region level (wide)\", cols_to_check)\n",
    "\n",
    "    # 1B) Spotchecks\n",
    "    spotcheck_copies(df_icb, \"ICB (wide)\")\n",
    "    spotcheck_copies(df_region, \"Region (wide)\")\n",
    "    spotcheck_combos(df_icb, \"ICB (wide)\")\n",
    "    spotcheck_combos(df_region, \"Region (wide)\")\n",
    "\n",
    "    # 1C) Rate bounds\n",
    "    rate_bounds(df_icb, \"ICB (wide)\")\n",
    "    rate_bounds(df_region, \"Region (wide)\")\n",
    "else:\n",
    "    print(\"\\nWide table unavailable — skipping column-based checks.\")\n",
    "\n",
    "# ---------- 2) If we have the long OPRT table, check presence of these metrics by name ----------\n",
    "if df_long is not None:\n",
    "    print(\"\\n--- OPRT long checks (by metric name) ---\")\n",
    "    target_metric_names = [  # these are expected as OPRT_Metric_Name entries if they exist upstream\n",
    "        \"All_DNA1\",\"All_DNA2\",\n",
    "        \"All_FU1\",\"All_FU2\",\"All_FU3\",\"All_FU4\",\"All_FU5\",\n",
    "        \"All_Total1\",\"All_Total2\",\"All_Total3\",\"All_Total4\",\"All_Total5\",\"All_Total6\",\n",
    "        \"Remote_Total1\",\"Remote_Total2\",\n",
    "        \"All_First_plus_All_First_DNA\",\n",
    "        \"All_FU_plus_All_FU_DNA\",\n",
    "        \"All_Total_plus_All_DNA\",\n",
    "        \"F2F_Total_plus_F2F_DNA\",\n",
    "        \"Remote_Total_plus_Remote_DNA\",\n",
    "        \"F2F_DNA\",\"F2F_DNA_Over_F2F_Total\"\n",
    "    ]\n",
    "    present = (df_long\n",
    "        .filter(F.col(\"Level\").isin(\"ICB\",\"Region\"))\n",
    "        .groupBy(\"OPRT_Metric_Name\")\n",
    "        .count()\n",
    "        .filter(F.col(\"OPRT_Metric_Name\").isin(target_metric_names))\n",
    "        .orderBy(\"OPRT_Metric_Name\"))\n",
    "\n",
    "    print(\"Presence of target metric names at ICB/Region in long table:\")\n",
    "    display(present)\n",
    "\n",
    "    # Null share by Level for those targets\n",
    "    mv_nulls = (df_long\n",
    "        .filter(F.col(\"Level\").isin(\"ICB\",\"Region\"))\n",
    "        .filter(F.col(\"OPRT_Metric_Name\").isin(target_metric_names))\n",
    "        .groupBy(\"Level\",\"OPRT_Metric_Name\")\n",
    "        .agg(F.count(\"*\").alias(\"rows\"),\n",
    "             F.sum(F.when(F.col(\"Metric_Value\").isNull(),1).otherwise(0)).alias(\"nulls\"),\n",
    "             F.round(F.sum(F.when(F.col(\"Metric_Value\").isNull(),1).otherwise(0))/F.count(\"*\")*100,2).alias(\"null_pct\"))\n",
    "        .orderBy(\"Level\",\"OPRT_Metric_Name\"))\n",
    "\n",
    "    print(\"Null share for those metrics (long):\")\n",
    "    display(mv_nulls)\n",
    "else:\n",
    "    print(\"\\nLong table unavailable — skipping OPRT checks.\")\n",
    "\n",
    "print(\"\\n=== QA complete. ===\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b26eaa7-0010-433f-b2f7-bead2585dd11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#20 sanity check on nulls in aggregation\n",
    "from pyspark.sql import functions as F\n",
    "df = spark.read.parquet(\"/mnt/output/opa_final_all_levels\")\n",
    "\n",
    "for lvl in [\"ICB\",\"Region\"]:\n",
    "    d = df.filter(F.col(\"Level\")==lvl)\n",
    "    n_null = d.filter(F.col(\"F2F_DNA_Over_F2F_Total\").isNull()).count()\n",
    "    n_zero = d.filter(((F.col(\"F2F_Total\").isNull()) | (F.col(\"F2F_Total\")==0)) &\n",
    "                      ((F.col(\"F2F_DNA\").isNull())  | (F.col(\"F2F_DNA\")==0))).count()\n",
    "    print(lvl, \"NULL rates:\", n_null, \"| zero denominators:\", n_zero)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "156c85e8-7836-4ddc-8b9b-c39c915e40df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#21 Saving the file to the lake mart for QA (filtered for a small sample)\n",
    "df_sample = df_with_id.filter(\n",
    "    (F.col(\"Der_Activity_Month_Date\") == \"2025-07-31\") & \n",
    "    (F.col(\"Adj_Org_Code_Final\") == \"RH8\")\n",
    "    #(F.col(\"ICB\") == \"QE1\")\n",
    "    #(F.col(\"Region_Code\") == \"Y59\")\n",
    ")\n",
    "df_sample_count = df_sample.count()\n",
    "#print(f\"Number of rows in filtered sample: {df_sample_count}\")\n",
    "\n",
    "df_sample.coalesce(1).write \\\n",
    "    .format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/Projects/OP_QA_STP_Region_new_metrics_short_RH8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1125d19-3066-4171-b99c-95955e2286a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#22 Saving the complete file to the lake mart for QA\n",
    "df_with_id.coalesce(1).write \\\n",
    "    .format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/Projects/OP_QA_STP_Region_new_metrics_new_news.csv\")\n",
    "\n",
    "#display(df_with_id.limit(10))\n",
    "#print(f\"Number of rows in df_with_id: {df_with_id.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1456ecc5-0235-47d9-a62e-39906eb80b92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#23 lower_bench mark, saving the file to the lake mart for QA \n",
    "opa_final_with_remote_benchmark.coalesce(1).write \\\n",
    "    .format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/Projects/lower_bm_new.csv\")\n",
    "\n",
    "#display(opa_final_with_remote_benchmark.limit(10))\n",
    "#print(f\"Number of rows in opa_final_with_remote_benchmark: {opa_final_with_remote_benchmark.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e918b0f-51ad-49d8-9595-f9954b2c5d9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#24 dna_opp, saving the file to the lake mart for QA \n",
    "dna_opp_df.coalesce(1).write \\\n",
    "    .format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .save(\"abfss://analytics-projects@udalstdataanalysisprod.dfs.core.windows.net/ElectiveRecovery/Projects/DNA_Opportunities_New.csv\")\n",
    "\n",
    "#display(dna_opp_df.limit(10))\n",
    "#print(f\"Number of rows in dna_opp_df: {dna_opp_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad8f098e-72cf-4778-982b-c3020bd3aba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('/Workspace/Repos/MHS-analytics/MHS-analytics/Python_Packages')\n",
    "# from mhs_import import MHS_IngestionHub\n",
    "# from mhs_db_config import INSERT, DELETE\n",
    "\n",
    "# def main():\n",
    "#     max_date = df_sample.agg(F.max(\"reportingDate\")).collect()[0][0]\n",
    "#     desc = f\"df_sample_{max_date}\"\n",
    "#     InjectionHub(df_sample, desc, True)\n",
    "#     print(\"df_sample sent to InjectionHub\")\n",
    "\n",
    "# def InjectionHub(dfih, desc, tf):\n",
    "#     sdf = dfih.withColumn('reportingDate', F.to_date('reportingDate'))\n",
    "#     display(sdf.sample(False, 0.01))\n",
    "#     display(sdf.dtypes)\n",
    "#     MHS_IngestionHub.upload(\n",
    "#         mhs_df=sdf,\n",
    "#         description=desc,\n",
    "#         loaded_by=\"steven.evans4@nhs.net\",\n",
    "#         mhs_mode=INSERT,\n",
    "#         skip_existing_data_check=tf\n",
    "#     )\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01f5a207-1000-4365-9ce9-e6e91057ca2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql import Row\n",
    "\n",
    "#col_names = opa_final_with_added_metrics.columns\n",
    "#df_col_names = spark.createDataFrame([Row(Column_Name=c) for c in col_names])\n",
    "#display(df_col_names)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "7 Re-shaping the data v4",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
